{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JtK9KJCP3H36"
      },
      "source": [
        "# Inteligencia Artificial - ESIS - UNJBG\n",
        "## Semana 3: Modelos Lineales y Autodiferenciación 2/3\n",
        "### Docente: MSc.(c) Israel N. Chaparro-Cruz\n",
        "**Basado en: Week 1, Day 2: Linear Deep Learning, By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Saeed Salehi, Andrew Saxe\n",
        "\n",
        "__Content reviewers:__ Polina Turishcheva, Antoine De Comite, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Anoop Kulkarni\n",
        "\n",
        "__Production editors:__ Khalid Almubarak, Gagana B, Spiros Chavlis\n",
        "\n",
        "__Content Traduction:__ Israel N. Chaparro-Cruz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "N2FLn7dg3H4F"
      },
      "source": [
        "---\n",
        "# Objetivos del tutorial\n",
        "\n",
        "* Panorama acerca del entrenamiento\n",
        "* El efecto de la profundidad\n",
        "* Elección de la tasa de aprendizaje\n",
        "* La inicialización es importante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "om7_1Z603H4J"
      },
      "source": [
        "---\n",
        "# Configuración\n",
        "\n",
        "Este es un tutorial sin GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "2QxthTme3H4M"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "oAmGmZnV3H4N"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "\n",
        "from ipywidgets import interact, IntSlider, FloatSlider, fixed\n",
        "from ipywidgets import HBox, interactive_output, ToggleButton, Layout\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "6isOn6303H4P"
      },
      "outputs": [],
      "source": [
        "# @title Plotting functions\n",
        "\n",
        "def plot_x_y_(x_t_, y_t_, x_ev_, y_ev_, loss_log_, weight_log_):\n",
        "  \"\"\"\n",
        "  Plot train data and test results\n",
        "\n",
        "  Args:\n",
        "  x_t_: np.ndarray\n",
        "    Training dataset\n",
        "  y_t_: np.ndarray\n",
        "    Ground truth corresponding to training dataset\n",
        "  x_ev_: np.ndarray\n",
        "    Evaluation set\n",
        "  y_ev_: np.ndarray\n",
        "    ShallowNarrowNet predictions\n",
        "  loss_log_: list\n",
        "    Training loss records\n",
        "  weight_log_: list\n",
        "    Training weight records (evolution of weights)\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.scatter(x_t_, y_t_, c='r', label='training data')\n",
        "  plt.plot(x_ev_, y_ev_, c='b', label='test results', linewidth=2)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.plot(loss_log_, c='r')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('mean squared error')\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.plot(weight_log_)\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('weights')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_vector_field(what, init_weights=None):\n",
        "  \"\"\"\n",
        "  Helper function to plot vector fields\n",
        "\n",
        "  Args:\n",
        "    what: string\n",
        "      If \"all\", plot vectors, trajectories and loss function\n",
        "      If \"vectors\", plot vectors\n",
        "      If \"trajectory\", plot trajectories\n",
        "      If \"loss\", plot loss function\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  n_epochs=40\n",
        "  lr=0.15\n",
        "  x_pos = np.linspace(2.0, 0.5, 100, endpoint=True)\n",
        "  y_pos = 1. / x_pos\n",
        "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
        "  zz = np.empty_like(xx)\n",
        "  x, y = xx[:, 0], yy[0]\n",
        "\n",
        "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
        "\n",
        "  cmap = matplotlib.cm.plasma\n",
        "  plt.figure(figsize=(8, 7))\n",
        "  ax = plt.gca()\n",
        "\n",
        "  if what == 'all' or what == 'vectors':\n",
        "    for i, a in enumerate(x):\n",
        "      for j, b in enumerate(y):\n",
        "        temp_model = ShallowNarrowLNN([a, b])\n",
        "        da, db = temp_model.dloss_dw(x_temp, y_temp)\n",
        "        zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
        "        scale = min(40 * np.sqrt(da**2 + db**2), 50)\n",
        "        ax.quiver(a, b, - da, - db, scale=scale, color=cmap(np.sqrt(da**2 + db**2)))\n",
        "\n",
        "  if what == 'all' or what == 'trajectory':\n",
        "    if init_weights is None:\n",
        "      for init_weights in [[0.5, -0.5], [0.55, -0.45], [-1.8, 1.7]]:\n",
        "        temp_model = ShallowNarrowLNN(init_weights)\n",
        "        _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
        "        ax.scatter(temp_records[:, 0], temp_records[:, 1],\n",
        "                    c=np.arange(len(temp_records)), cmap='Greys')\n",
        "        ax.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
        "        ax.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
        "    else:\n",
        "      temp_model = ShallowNarrowLNN(init_weights)\n",
        "      _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
        "      ax.scatter(temp_records[:, 0], temp_records[:, 1],\n",
        "                  c=np.arange(len(temp_records)), cmap='Greys')\n",
        "      ax.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
        "      ax.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
        "\n",
        "  if what == 'all' or what == 'loss':\n",
        "    contplt = ax.contourf(x, y, np.log(zz+0.001), zorder=-1, cmap='coolwarm', levels=100)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(contplt, cax=cax)\n",
        "    cbar.set_label('log (Loss)')\n",
        "\n",
        "  ax.set_xlabel(\"$w_1$\")\n",
        "  ax.set_ylabel(\"$w_2$\")\n",
        "  ax.set_xlim(-1.9, 1.9)\n",
        "  ax.set_ylim(-1.9, 1.9)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_landscape():\n",
        "  \"\"\"\n",
        "  Helper function to plot loss landscapes\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
        "\n",
        "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
        "  zz = np.empty_like(xx)\n",
        "  x, y = xx[:, 0], yy[0]\n",
        "\n",
        "  for i, a in enumerate(x):\n",
        "    for j, b in enumerate(y):\n",
        "      temp_model = ShallowNarrowLNN([a, b])\n",
        "      zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
        "\n",
        "  temp_model = ShallowNarrowLNN([-1.8, 1.7])\n",
        "  loss_rec_1, w_rec_1 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
        "\n",
        "  temp_model = ShallowNarrowLNN([1.5, -1.5])\n",
        "  loss_rec_2, w_rec_2 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
        "\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  ax = plt.subplot(1, 1, 1, projection='3d')\n",
        "  ax.plot_surface(xx, yy, np.log(zz+0.5), cmap='coolwarm', alpha=0.5)\n",
        "  ax.scatter3D(w_rec_1[:, 0], w_rec_1[:, 1], np.log(loss_rec_1+0.5),\n",
        "                c='k', s=50, zorder=9)\n",
        "  ax.scatter3D(w_rec_2[:, 0], w_rec_2[:, 1], np.log(loss_rec_2+0.5),\n",
        "                c='k', s=50, zorder=9)\n",
        "  plt.axis(\"off\")\n",
        "  ax.view_init(45, 260)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def depth_widget(depth):\n",
        "  \"\"\"\n",
        "  Simulate parameter in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  if depth == 0:\n",
        "    depth_lr_init_interplay(depth, 0.02, 0.9)\n",
        "  else:\n",
        "    depth_lr_init_interplay(depth, 0.01, 0.9)\n",
        "\n",
        "\n",
        "def lr_widget(lr):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth_lr_init_interplay(50, lr, 0.9)\n",
        "\n",
        "\n",
        "def depth_lr_interplay(depth, lr):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth_lr_init_interplay(depth, lr, 0.9)\n",
        "\n",
        "\n",
        "def depth_lr_init_interplay(depth, lr, init_weights):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "    init_weights: list\n",
        "      Specifies initial weights of the network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  n_epochs = 600\n",
        "\n",
        "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
        "  model = DeepNarrowLNN(np.full((1, depth+1), init_weights))\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
        "            linewidth=3.0, c='m')\n",
        "\n",
        "  plt.title(\"Training a {}-layer LNN with\"\n",
        "  \" $\\eta=${} initialized with $w_i=${}\".format(depth, lr, init_weights), pad=15)\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('Log mean squared error')\n",
        "  plt.ylim(0.001, 1.0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_init_effect():\n",
        "  \"\"\"\n",
        "  Helper function to plot evolution of log mean\n",
        "  squared error over epochs\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth = 15\n",
        "  n_epochs = 250\n",
        "  lr = 0.02\n",
        "\n",
        "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  for init_w in np.arange(0.7, 1.09, 0.05):\n",
        "      model = DeepNarrowLNN(np.full((1, depth), init_w))\n",
        "      plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
        "              linewidth=3.0, label=\"initial weights {:.2f}\".format(init_w))\n",
        "  plt.title(\"Training a {}-layer narrow LNN with $\\eta=${}\".format(depth, lr), pad=15)\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('Log mean squared error')\n",
        "  plt.legend(loc='lower left', ncol=4)\n",
        "  plt.ylim(0.001, 1.0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "class InterPlay:\n",
        "  \"\"\"\n",
        "  Class specifying parameters for widget\n",
        "  exploring relationship between the depth\n",
        "  and optimal learning rate\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize parameters for InterPlay\n",
        "\n",
        "    Args:\n",
        "      None\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    self.lr = [None]\n",
        "    self.depth = [None]\n",
        "    self.success = [None]\n",
        "    self.min_depth, self.max_depth = 5, 65\n",
        "    self.depth_list = np.arange(10, 61, 10)\n",
        "    self.i_depth = 0\n",
        "    self.min_lr, self.max_lr = 0.001, 0.105\n",
        "    self.n_epochs = 600\n",
        "    self.x_train, self.y_train = gen_samples(100, 2.0, 0.1)\n",
        "    self.converged = False\n",
        "    self.button = None\n",
        "    self.slider = None\n",
        "\n",
        "  def train(self, lr, update=False, init_weights=0.9):\n",
        "    \"\"\"\n",
        "    Train network associated with InterPlay\n",
        "\n",
        "    Args:\n",
        "      lr: float\n",
        "        Specifies learning rate within network\n",
        "      init_weights: float\n",
        "        Specifies initial weights of the network [default: 0.9]\n",
        "      update: boolean\n",
        "        If true, show updates on widget\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    if update and self.converged and self.i_depth < len(self.depth_list):\n",
        "      depth = self.depth_list[self.i_depth]\n",
        "      self.plot(depth, lr)\n",
        "      self.i_depth += 1\n",
        "      self.lr.append(None)\n",
        "      self.depth.append(None)\n",
        "      self.success.append(None)\n",
        "      self.converged = False\n",
        "      self.slider.value = 0.005\n",
        "      if self.i_depth < len(self.depth_list):\n",
        "        self.button.value = False\n",
        "        self.button.description = 'Explore!'\n",
        "        self.button.disabled = True\n",
        "        self.button.button_style = 'Danger'\n",
        "      else:\n",
        "        self.button.value = False\n",
        "        self.button.button_style = ''\n",
        "        self.button.disabled = True\n",
        "        self.button.description = 'Done!'\n",
        "      time.sleep(1.0)\n",
        "\n",
        "    elif self.i_depth < len(self.depth_list):\n",
        "      depth = self.depth_list[self.i_depth]\n",
        "      # Additional assert: self.min_depth <= depth <= self.max_depth\n",
        "      assert self.min_lr <= lr <= self.max_lr\n",
        "      self.converged = False\n",
        "\n",
        "      model = DeepNarrowLNN(np.full((1, depth), init_weights))\n",
        "      self.losses = np.array(model.train(self.x_train, self.y_train, lr, self.n_epochs))\n",
        "      if np.any(self.losses < 1e-2):\n",
        "        success = np.argwhere(self.losses < 1e-2)[0][0]\n",
        "        if np.all((self.losses[success:] < 1e-2)):\n",
        "          self.converged = True\n",
        "          self.success[-1] = success\n",
        "          self.lr[-1] = lr\n",
        "          self.depth[-1] = depth\n",
        "          self.button.disabled = False\n",
        "          self.button.button_style = 'Success'\n",
        "          self.button.description = 'Register!'\n",
        "        else:\n",
        "          self.button.disabled = True\n",
        "          self.button.button_style = 'Danger'\n",
        "          self.button.description = 'Explore!'\n",
        "      else:\n",
        "        self.button.disabled = True\n",
        "        self.button.button_style = 'Danger'\n",
        "        self.button.description = 'Explore!'\n",
        "      self.plot(depth, lr)\n",
        "\n",
        "  def plot(self, depth, lr):\n",
        "    \"\"\"\n",
        "    Plot following subplots:\n",
        "    a. Log mean squared error vs Epochs\n",
        "    b. Learning time vs Depth\n",
        "    c. Optimal learning rate vs Depth\n",
        "\n",
        "    Args:\n",
        "      depth: int\n",
        "        Specifies depth of network\n",
        "      lr: float\n",
        "        Specifies learning rate of network\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    fig = plt.figure(constrained_layout=False, figsize=(10, 8))\n",
        "    gs = fig.add_gridspec(2, 2)\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "    ax1.plot(self.losses, linewidth=3.0, c='m')\n",
        "    ax1.set_title(\"Training a {}-layer LNN with\"\n",
        "    \" $\\eta=${}\".format(depth, lr), pad=15, fontsize=16)\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.set_xlabel('epochs')\n",
        "    ax1.set_ylabel('Log mean squared error')\n",
        "    ax1.set_ylim(0.001, 1.0)\n",
        "\n",
        "    ax2.set_xlim(self.min_depth, self.max_depth)\n",
        "    ax2.set_ylim(-10, self.n_epochs)\n",
        "    ax2.set_xlabel('Depth')\n",
        "    ax2.set_ylabel('Learning time (Epochs)')\n",
        "    ax2.set_title(\"Learning time vs depth\", fontsize=14)\n",
        "    ax2.scatter(np.array(self.depth), np.array(self.success), c='r')\n",
        "\n",
        "    ax3.set_xlim(self.min_depth, self.max_depth)\n",
        "    ax3.set_ylim(self.min_lr, self.max_lr)\n",
        "    ax3.set_xlabel('Depth')\n",
        "    ax3.set_ylabel('Optimal learning rate')\n",
        "    ax3.set_title(\"Empirically optimal $\\eta$ vs depth\", fontsize=14)\n",
        "    ax3.scatter(np.array(self.depth), np.array(self.lr), c='r')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "o4CaZ0fI3H4Z"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "\n",
        "def gen_samples(n, a, sigma):\n",
        "  \"\"\"\n",
        "  Generates n samples with\n",
        "  `y = z * x + noise(sigma)` linear relation.\n",
        "\n",
        "  Args:\n",
        "    n : int\n",
        "      Number of datapoints within sample\n",
        "    a : float\n",
        "      Offset of x\n",
        "    sigma : float\n",
        "      Standard deviation of distribution\n",
        "\n",
        "  Returns:\n",
        "    x : np.array\n",
        "      if sigma > 0, x = random values\n",
        "      else, x = evenly spaced numbers over a specified interval.\n",
        "    y : np.array\n",
        "      y = z * x + noise(sigma)\n",
        "  \"\"\"\n",
        "  assert n > 0\n",
        "  assert sigma >= 0\n",
        "\n",
        "  if sigma > 0:\n",
        "    x = np.random.rand(n)\n",
        "    noise = np.random.normal(scale=sigma, size=(n))\n",
        "    y = a * x + noise\n",
        "  else:\n",
        "    x = np.linspace(0.0, 1.0, n, endpoint=True)\n",
        "    y = a * x\n",
        "  return x, y\n",
        "\n",
        "\n",
        "class ShallowNarrowLNN:\n",
        "  \"\"\"\n",
        "  Shallow and narrow (one neuron per layer)\n",
        "  linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_ws):\n",
        "    \"\"\"\n",
        "    Initialize parameters of ShallowNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      init_ws: initial weights as a list\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    assert isinstance(init_ws, list)\n",
        "    assert len(init_ws) == 2\n",
        "    self.w1 = init_ws[0]\n",
        "    self.w2 = init_ws[1]\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward pass through network y = x * w1 * w2\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Input data\n",
        "\n",
        "    Returns:\n",
        "      y: np.ndarray\n",
        "        y = x * w1 * w2\n",
        "    \"\"\"\n",
        "    y = x * self.w1 * self.w2\n",
        "    return y\n",
        "\n",
        "  def loss(self, y_p, y_t):\n",
        "    \"\"\"\n",
        "    Mean squared error (L2)\n",
        "    with 1/2 for convenience\n",
        "\n",
        "    Args:\n",
        "      y_p: np.ndarray\n",
        "        Network Predictions\n",
        "      y_t: np.ndarray\n",
        "        Targets\n",
        "\n",
        "    Returns:\n",
        "      mse: float\n",
        "        Average mean squared error\n",
        "    \"\"\"\n",
        "    assert y_p.shape == y_t.shape\n",
        "    mse = ((y_t - y_p)**2).mean()\n",
        "    return mse\n",
        "\n",
        "  def dloss_dw(self, x, y_t):\n",
        "    \"\"\"\n",
        "    Partial derivative of loss with respect to weights\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Dataset\n",
        "      y_t : np.array\n",
        "        Corresponding Ground Truth\n",
        "\n",
        "    Returns:\n",
        "      dloss_dw1: float\n",
        "        -mean(2 * self.w2 * x * Error)\n",
        "      dloss_dw2: float\n",
        "        -mean(2 * self.w1 * x * Error)\n",
        "    \"\"\"\n",
        "    assert x.shape == y_t.shape\n",
        "    Error = y_t - self.w1 * self.w2 * x\n",
        "    dloss_dw1 = - (2 * self.w2 * x * Error).mean()\n",
        "    dloss_dw2 = - (2 * self.w1 * x * Error).mean()\n",
        "    return dloss_dw1, dloss_dw2\n",
        "\n",
        "  def train(self, x, y_t, eta, n_ep):\n",
        "    \"\"\"\n",
        "    Gradient descent algorithm\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Dataset\n",
        "      y_t : np.array\n",
        "        Corrsponding target\n",
        "      eta: float\n",
        "        Learning rate\n",
        "      n_ep : int\n",
        "        Number of epochs\n",
        "\n",
        "    Returns:\n",
        "      loss_records: np.ndarray\n",
        "        Log of loss per epoch\n",
        "      weight_records: np.ndarray\n",
        "        Log of weights per epoch\n",
        "    \"\"\"\n",
        "    assert x.shape == y_t.shape\n",
        "\n",
        "    loss_records = np.empty(n_ep)  # Pre allocation of loss records\n",
        "    weight_records = np.empty((n_ep, 2))  # Pre allocation of weight records\n",
        "\n",
        "    for i in range(n_ep):\n",
        "      y_p = self.forward(x)\n",
        "      loss_records[i] = self.loss(y_p, y_t)\n",
        "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_t)\n",
        "      self.w1 -= eta * dloss_dw1\n",
        "      self.w2 -= eta * dloss_dw2\n",
        "      weight_records[i] = [self.w1, self.w2]\n",
        "\n",
        "    return loss_records, weight_records\n",
        "\n",
        "\n",
        "class DeepNarrowLNN:\n",
        "  \"\"\"\n",
        "  Deep but thin (one neuron per layer)\n",
        "  linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_ws):\n",
        "    \"\"\"\n",
        "    Initialize parameters of DeepNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      init_ws: np.ndarray\n",
        "        Initial weights as a numpy array\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    self.n = init_ws.size\n",
        "    self.W = init_ws.reshape(1, -1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of DeepNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      y: np.array\n",
        "        Product of weights over input features\n",
        "    \"\"\"\n",
        "    y = np.prod(self.W) * x\n",
        "    return y\n",
        "\n",
        "  def loss(self, y_t, y_p):\n",
        "    \"\"\"\n",
        "    Mean squared error (L2 loss)\n",
        "\n",
        "    Args:\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      y_p : np.array\n",
        "        Network's predictions\n",
        "\n",
        "    Returns:\n",
        "      mse: float\n",
        "        Mean squared error\n",
        "    \"\"\"\n",
        "    assert y_p.shape == y_t.shape\n",
        "    mse = ((y_t - y_p)**2 / 2).mean()\n",
        "    return mse\n",
        "\n",
        "  def dloss_dw(self, x, y_t, y_p):\n",
        "    \"\"\"\n",
        "    Analytical gradient of weights\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input features\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      y_p : np.array\n",
        "        Network Predictions\n",
        "\n",
        "    Returns:\n",
        "      dW: np.ndarray\n",
        "        Analytical gradient of weights\n",
        "    \"\"\"\n",
        "    E = y_t - y_p  # i.e., y_t - x * np.prod(self.W)\n",
        "    Ex = np.multiply(x, E).mean()\n",
        "    Wp = np.prod(self.W) / (self.W + 1e-9)\n",
        "    dW = - Ex * Wp\n",
        "    return dW\n",
        "\n",
        "  def train(self, x, y_t, eta, n_epochs):\n",
        "    \"\"\"\n",
        "    Training using gradient descent\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Features\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      eta: float\n",
        "        Learning rate\n",
        "      n_epochs : int\n",
        "        Number of epochs\n",
        "\n",
        "    Returns:\n",
        "      loss_records: np.ndarray\n",
        "        Log of loss over epochs\n",
        "    \"\"\"\n",
        "    loss_records = np.empty(n_epochs)\n",
        "    loss_records[:] = np.nan\n",
        "    for i in range(n_epochs):\n",
        "      y_p = self.forward(x)\n",
        "      loss_records[i] = self.loss(y_t, y_p).mean()\n",
        "      dloss_dw = self.dloss_dw(x, y_t, y_p)\n",
        "      if np.isnan(dloss_dw).any() or np.isinf(dloss_dw).any():\n",
        "        return loss_records\n",
        "      self.W -= eta * dloss_dw\n",
        "    return loss_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "6aj5KEJY3H4c"
      },
      "outputs": [],
      "source": [
        "#@title Set random seed\n",
        "\n",
        "#@markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# For DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "GbjdOftY3H4e"
      },
      "outputs": [],
      "source": [
        "#@title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# Inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"GPU is not enabled in this notebook. \\n\"\n",
        "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook. \\n\"\n",
        "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "mDM4c91h3H4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414579f0-ea6d-46b4-e11f-1d9b2ce3d040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is not enabled in this notebook. \n",
            "If you want to enable it, in the menu under `Runtime` -> \n",
            "`Hardware accelerator.` and select `GPU` from the dropdown menu\n"
          ]
        }
      ],
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gQro91xA3H4h"
      },
      "source": [
        "---\n",
        "# Sección 1: Una red neuronal lineal poco profunda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ghmk6_bA3H4i"
      },
      "source": [
        "## Sección 1.1: Una red lineal poco profunda y estrecha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aFbJ4b7B3H4j"
      },
      "source": [
        "Para entender mejor el comportamiento del entrenamiento de redes neuronales con descenso de gradiente, empezamos con el caso increíblemente sencillo de una red neuronal lineal estrecha y poco profunda, ya que los modelos más avanzados son imposibles de diseccionar y comprender con nuestras herramientas matemáticas actuales.\n",
        "\n",
        "El modelo que utilizamos tiene una capa oculta, con una sola neurona, y dos pesos. Consideramos el error al cuadrado (o pérdida L2) como función de coste. Como ya habrá adivinado, podemos visualizar el modelo como una red neuronal:\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/shallow_narrow_nn.png\" width=\"400\"/></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "o por su gráfico de cálculo:\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/shallow_narrow.png\" alt=\"Shallow Narrow Graph\" width=\"400\"/></center>\n",
        "\n",
        "o, en raras ocasiones, incluso como una cartografía razonablemente compacta:\n",
        "\n",
        "$$ loss = (y - w_1 \\cdot w_2 \\cdot x)^2 $$\n",
        "\n",
        "<br/>\n",
        "\n",
        "Implementar una red neuronal desde cero sin utilizar ninguna herramienta de diferenciación automática es raramente necesario. Por lo tanto, los dos siguientes ejercicios son **Bonus** (opcionales). Por favor, ignórelos si tiene algún límite de tiempo o presión y continúe con la Sección 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TNhkkPDp3H4j"
      },
      "source": [
        "### Ejercicio analítico 1.1: Gradientes de pérdidas (opcional)\n",
        "\n",
        "Una vez más, le pedimos que calcule los gradientes de la red de forma analítica, ya que los necesitará para el siguiente ejercicio. Entendemos lo molesto que es esto.\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_1}} = ?$\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_2}} = ?$\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "#### Solución\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_1}} = -2 \\cdot w_2 \\cdot x \\cdot (y - w_1 \\cdot w_2 \\cdot x)$\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_2}} = -2 \\cdot w_1 \\cdot x \\cdot (y - w_1 \\cdot w_2 \\cdot x)$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EdlDxyC63H4k"
      },
      "source": [
        "### Ejercicio de codificación 1.1: Implementar una LNN estrecha simple (Opcional)\n",
        "\n",
        "A continuación, te pedimos que implementes el pase `forward` para nuestro modelo desde cero sin usar PyTorch.\n",
        "\n",
        "Además, aunque nuestro modelo obtiene una única característica de entrada y emite una única predicción, podríamos calcular la pérdida y realizar el entrenamiento para múltiples muestras a la vez. Esta es la práctica común para las redes neuronales, ya que los ordenadores son increíblemente rápidos haciendo operaciones matriciales (o tensoriales) en lotes de datos, en lugar de procesar las muestras de una en una a través de bucles `for`. Por lo tanto, para la función \"pérdida\", por favor, implemente el error cuadrático medio (MSE), y ajuste sus gradientes analíticos en consecuencia cuando implemente la función \"dloss_dw\".\n",
        "\n",
        "Por último, completar la función `train` para el algoritmo de descenso de gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla loss (\\mathbf{w}^{(t)})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "aueV78_H3H4k"
      },
      "outputs": [],
      "source": [
        "class ShallowNarrowExercise:\n",
        "  \"\"\"\n",
        "  Shallow and narrow (one neuron per layer) linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_weights):\n",
        "    \"\"\"\n",
        "    Initialize parameters of ShallowNarrow Net\n",
        "\n",
        "    Args:\n",
        "      init_weights: list\n",
        "        Initial weights\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    assert isinstance(init_weights, (list, np.ndarray, tuple))\n",
        "    assert len(init_weights) == 2\n",
        "    self.w1 = init_weights[0]\n",
        "    self.w2 = init_weights[1]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward pass through netwrok y = x * w1 * w2\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "\n",
        "    Returns:\n",
        "      y: np.ndarray\n",
        "        Neural network output (predictions)\n",
        "    \"\"\"\n",
        "    #################################################\n",
        "    ## Implement the forward pass to calculate prediction\n",
        "    ## Note that prediction is not the loss\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
        "    #################################################\n",
        "    y = ...\n",
        "    return y\n",
        "\n",
        "\n",
        "  def dloss_dw(self, x, y_true):\n",
        "    \"\"\"\n",
        "    Gradient of loss with respect to weights\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "      y_true: np.ndarray\n",
        "        True labels\n",
        "\n",
        "    Returns:\n",
        "      dloss_dw1: float\n",
        "        Mean gradient of loss with respect to w1\n",
        "      dloss_dw2: float\n",
        "        Mean gradient of loss with respect to w2\n",
        "    \"\"\"\n",
        "    assert x.shape == y_true.shape\n",
        "    #################################################\n",
        "    ## Implement the gradient computation function\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Gradient of Loss `dloss_dw`\")\n",
        "    #################################################\n",
        "    dloss_dw1 = ...\n",
        "    dloss_dw2 = ...\n",
        "    return dloss_dw1, dloss_dw2\n",
        "\n",
        "\n",
        "  def train(self, x, y_true, lr, n_ep):\n",
        "    \"\"\"\n",
        "    Training with Gradient descent algorithm\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "      y_true: np.ndarray\n",
        "        True labels\n",
        "      lr: float\n",
        "        Learning rate\n",
        "      n_ep: int\n",
        "        Number of epochs (training iterations)\n",
        "\n",
        "    Returns:\n",
        "      loss_records: list\n",
        "        Training loss records\n",
        "      weight_records: list\n",
        "        Training weight records (evolution of weights)\n",
        "    \"\"\"\n",
        "    assert x.shape == y_true.shape\n",
        "\n",
        "    loss_records = np.empty(n_ep)  # Pre allocation of loss records\n",
        "    weight_records = np.empty((n_ep, 2))  # Pre allocation of weight records\n",
        "\n",
        "    for i in range(n_ep):\n",
        "      y_prediction = self.forward(x)\n",
        "      loss_records[i] = loss(y_prediction, y_true)\n",
        "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_true)\n",
        "      #################################################\n",
        "      ## Implement the gradient descent step\n",
        "      # Complete the function and remove or comment the line below\n",
        "      raise NotImplementedError(\"Training loop `train`\")\n",
        "      #################################################\n",
        "      self.w1 -= ...\n",
        "      self.w2 -= ...\n",
        "      weight_records[i] = [self.w1, self.w2]\n",
        "\n",
        "    return loss_records, weight_records\n",
        "\n",
        "\n",
        "def loss(y_prediction, y_true):\n",
        "  \"\"\"\n",
        "  Mean squared error\n",
        "\n",
        "  Args:\n",
        "    y_prediction: np.ndarray\n",
        "      Model output (prediction)\n",
        "    y_true: np.ndarray\n",
        "      True label\n",
        "\n",
        "  Returns:\n",
        "    mse: np.ndarray\n",
        "      Mean squared error loss\n",
        "  \"\"\"\n",
        "  assert y_prediction.shape == y_true.shape\n",
        "  #################################################\n",
        "  ## Implement the MEAN squared error\n",
        "  # Complete the function and remove or comment the line below\n",
        "  raise NotImplementedError(\"Loss function `loss`\")\n",
        "  #################################################\n",
        "  mse = ...\n",
        "  return mse\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "n_epochs = 211\n",
        "learning_rate = 0.02\n",
        "initial_weights = [1.4, -1.6]\n",
        "x_train, y_train = gen_samples(n=73, a=2.0, sigma=0.2)\n",
        "x_eval = np.linspace(0.0, 1.0, 37, endpoint=True)\n",
        "## Uncomment to run\n",
        "# sn_model = ShallowNarrowExercise(initial_weights)\n",
        "# loss_log, weight_log = sn_model.train(x_train, y_train, learning_rate, n_epochs)\n",
        "# y_eval = sn_model.forward(x_eval)\n",
        "# plot_x_y_(x_train, y_train, x_eval, y_eval, loss_log, weight_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "7GPWuV-p3H4l"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial2_Solution_b008d048.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1696.0 height=544.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/W1D2_Tutorial2_Solution_b008d048_1.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "o4ohXAXE3H4m"
      },
      "source": [
        "## Sección 1.2: Panorama de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8r0zhglE3H4n"
      },
      "source": [
        "Como ya te habrás preguntado, podemos encontrar analíticamente $w_1$ y $w_2$ sin utilizar el descenso de gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "w_1 \\cdot w_2 = \\dfrac{y}{x}\n",
        "\\end{equation}\n",
        "\n",
        "De hecho, podemos representar los gradientes, la función de pérdida y todas las soluciones posibles en una sola figura. En este ejemplo, utilizamos la cartografía $y = 1x$:\n",
        "\n",
        "**Cinta azul**: muestra todas las soluciones posibles: $~ w_1 w_2 = \\dfrac{y}{x} = \\dfrac{x}{x} = 1 \\Rightarrow w_1 = \\dfrac{1}{w_2}$\n",
        "\n",
        "**Fondo del contorno**: Muestra los valores de pérdida, siendo el rojo una pérdida mayor\n",
        "\n",
        "**Campo vectorial (flechas)**: muestra el campo vectorial del gradiente. Las flechas amarillas más grandes muestran gradientes más grandes, que corresponden a pasos más grandes por el descenso de gradiente.\n",
        "\n",
        "**Círculos de dispersión**: la trayectoria (evolución) de los pesos durante el entrenamiento para tres inicializaciones diferentes, con puntos azules marcando el inicio del entrenamiento y cruces rojas ( **x** ) marcando el final del entrenamiento. También puede probar sus propias inicializaciones (mantenga los valores iniciales entre `-2,0` y `2,0`) como se muestra aquí:\n",
        "\n",
        "```python\n",
        "plot_vector_field('all', [1.0, -1.0])\n",
        "```\n",
        "\n",
        "Por último, si la parcela está demasiado llena, no dude en pasar una de las siguientes cadenas como argumento:\n",
        "\n",
        "```python\n",
        "plot_vector_field('vectors')  # For vector field\n",
        "plot_vector_field('trajectory')  # For training trajectory\n",
        "plot_vector_field('loss')  # For loss contour\n",
        "```\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "Explora los dos gráficos siguientes. Prueba con diferentes valores iniciales.  ¿Puedes encontrar el punto de equilibrio? ¿Por qué el entrenamiento se ralentiza cerca de los mínimos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "9tXwsU2N3H4n"
      },
      "outputs": [],
      "source": [
        "plot_vector_field('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ygO5qCSW3H4n"
      },
      "source": [
        "Here, we also visualize the loss landscape in a 3-D plot, with two training trajectories for different initial conditions.\n",
        "Note: the trajectories from the 3D plot and the previous plot are independent and different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "UXypGK1g3H4o"
      },
      "outputs": [],
      "source": [
        "plot_loss_landscape()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "h6-_hzYa3H4p"
      },
      "source": [
        "---\n",
        "# Sección 2: Profundidad, tasa de aprendizaje e inicialización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "FDgA-vtI3H4p"
      },
      "source": [
        "Los modelos de aprendizaje profundo exitosos suelen ser desarrollados por un equipo de personas muy inteligentes, que pasan muchas horas \"ajustando\" los hiperparámetros de aprendizaje y encontrando inicializaciones eficaces. En esta sección, examinamos tres hiperparámetros básicos (pero a menudo no simples): la profundidad, la tasa de aprendizaje y la inicialización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sCSRLYIa3H4q"
      },
      "source": [
        "## Sección 2.1: El efecto de la profundidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iBGJhyyV3H4q"
      },
      "source": [
        "¿Por qué puede ser útil la profundidad? ¿Qué hace que una red o sistema de aprendizaje sea \"profundo\"? La realidad es que las redes neuronales poco profundas suelen ser incapaces de aprender funciones complejas debido a las limitaciones de los datos. En cambio, la profundidad parece mágica. La profundidad puede cambiar las funciones que una red puede representar, la forma en que una red aprende y cómo una red generaliza a datos no vistos. \n",
        "\n",
        "Veamos los retos que plantea la profundidad en el entrenamiento de una red neuronal. Imaginemos una red lineal de una sola entrada y una sola salida con 50 capas ocultas y una sola neurona por capa (es decir, una red neuronal profunda estrecha). La salida de la red es fácil de calcular:\n",
        "\n",
        "$$ prediction = x \\cdot w_1 \\cdot w_2 \\cdot \\cdot \\cdot w_{50} $$\n",
        "\n",
        "Si el valor inicial de todos los pesos es $w_i = 2$, la predicción para $x=1$ sería **explosiva**: $y_p = 2^{50} \\approx 1,1256 \\times 10^{15}$. Por otro lado, para pesos inicializados a $w_i = 0,5$, la salida es **desvaneciente**: $y_p = 0,5^{50} \\approx 8,88 \\times 10^{-16}$. Del mismo modo, si recordamos la regla de la cadena, a medida que el gráfico se hace más profundo, el número de elementos en la multiplicación de la cadena aumenta, lo que podría llevar a la explosión o desaparición de los gradientes. Para evitar estas vulnerabilidades numéricas que podrían perjudicar a nuestro algoritmo de entrenamiento, tenemos que entender el efecto de la profundidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mRCLyhyJ3H4r"
      },
      "source": [
        "### Demostración interactiva 2.1: Widget de profundidad\n",
        "\n",
        "Utilice el widget para explorar el impacto de la profundidad en la curva de entrenamiento (evolución de pérdidas) de una red neuronal profunda pero estrecha.\n",
        "\n",
        "**¡Piensa!**\n",
        "\n",
        "¿Qué redes se entrenaron más rápido? ¿Todas las redes acabaron por \"funcionar\" (converger)? ¿Cuál es la forma de su trayectoria de aprendizaje?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "8lkIS1LM3H4s"
      },
      "outputs": [],
      "source": [
        "# @markdown Asegúrate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(depth_widget,\n",
        "    depth = IntSlider(min=0, max=51,\n",
        "                      step=5, value=0,\n",
        "                      continuous_update=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Qi2i2mfi3H4t"
      },
      "source": [
        "## Sección 2.2: Elección de la tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JAq0oFIj3H4t"
      },
      "source": [
        "La tasa de aprendizaje es un hiperparámetro común para la mayoría de los algoritmos de optimización. ¿Cómo debemos fijarlo? A veces, la única opción es probar todas las posibilidades, pero a veces conocer algunas compensaciones clave nos ayudará a guiar nuestra búsqueda de buenos hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "QTeSn1v_3H4u"
      },
      "source": [
        "### Demostración interactiva 2.2: Widget de tasa de aprendizaje\n",
        "\n",
        "Aquí, fijamos la profundidad de la red en 50 capas. Utilice el widget para explorar el impacto de la tasa de aprendizaje $\\eta$ en la curva de entrenamiento (evolución de las pérdidas) de una red neuronal profunda pero estrecha.\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "¿Podemos decir que un mayor ritmo de aprendizaje siempre conduce a un aprendizaje más rápido? ¿Por qué no? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "2ZcwOD523H4u"
      },
      "outputs": [],
      "source": [
        "# @markdown Asegúrate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(lr_widget,\n",
        "    lr = FloatSlider(min=0.005, max=0.045, step=0.005, value=0.005,\n",
        "                     continuous_update=False, readout_format='.3f',\n",
        "                     description='eta'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DnQWhnP63H4v"
      },
      "source": [
        "## Sección 2.3: Profundidad vs. Ritmo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "owi8q-mx3H4w"
      },
      "source": [
        "### Demostración Interactiva 2.3: Profundidad y ritmo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-ZzTnf__3H4w"
      },
      "source": [
        "**Instrucción importante**\n",
        "El ejercicio comienza con 10 capas ocultas. Tu tarea es encontrar la tasa de aprendizaje que proporcione una convergencia (aprendizaje) rápida pero robusta. Cuando estés seguro de la tasa de aprendizaje, puedes **Registrar** la tasa de aprendizaje óptima para la profundidad dada. Una vez que se pulsa el botón de registro, se instala un modelo más profundo, de modo que se puede encontrar la siguiente tasa de aprendizaje óptima. El botón de registro se vuelve verde sólo cuando el entrenamiento converge, pero no implica la convergencia más rápida. Por último, ten paciencia :) los widgets son lentos.\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "¿Puedes explicar la relación entre la profundidad y la tasa de aprendizaje óptima?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "_gOwoegq3H4w"
      },
      "outputs": [],
      "source": [
        "# @markdown Asegúrate de ejecutar esta celda para habilitar el widget.\n",
        "intpl_obj = InterPlay()\n",
        "\n",
        "intpl_obj.slider = FloatSlider(min=0.005, max=0.105, step=0.005, value=0.005,\n",
        "                               layout=Layout(width='500px'),\n",
        "                               continuous_update=False,\n",
        "                               readout_format='.3f',\n",
        "                               description='eta')\n",
        "\n",
        "intpl_obj.button = ToggleButton(value=intpl_obj.converged, description='Register')\n",
        "\n",
        "widgets_ui = HBox([intpl_obj.slider, intpl_obj.button])\n",
        "widgets_out = interactive_output(intpl_obj.train,\n",
        "                                 {'lr': intpl_obj.slider,\n",
        "                                  'update': intpl_obj.button,\n",
        "                                  'init_weights': fixed(0.9)})\n",
        "\n",
        "display(widgets_ui, widgets_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "B4CflwoV3H4x"
      },
      "source": [
        "## Sección 2.4: Por qué es importante la inicialización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qRYN9f_J3H4y"
      },
      "source": [
        "Hemos visto, incluso en los casos más sencillos, que la profundidad puede ralentizar el aprendizaje. ¿Por qué? Por la regla de la cadena, los gradientes se multiplican por el peso actual en cada capa, por lo que el producto puede desaparecer o explotar. Por lo tanto, la inicialización de los pesos es un hiperparámetro fundamentalmente importante.\n",
        "\n",
        "Aunque en la práctica los valores iniciales para los parámetros aprendibles son a menudo muestreados de diferentes $\\mathcal{Uniform}$ o $\\mathcal{Normal}$ distribución de probabilidad, aquí utilizamos un único valor para todos los parámetros.\n",
        "\n",
        "La figura siguiente muestra el efecto de la inicialización en la velocidad de aprendizaje para la LNN profunda pero estrecha. Hemos excluido las inicializaciones que conducen a errores numéricos como `nan` o `inf`, que son la consecuencia de inicializaciones más pequeñas o más grandes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "91wfXMJB3H4z"
      },
      "outputs": [],
      "source": [
        "# @markdown ¡Asegúrate de ejecutar esta celda para ver la figura!\n",
        "\n",
        "plot_init_effect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lAzo2bt63H40"
      },
      "source": [
        "---\n",
        "# Resumen\n",
        "\n",
        "En el segundo tutorial, hemos aprendido qué es el paisaje de entrenamiento, y también hemos visto en profundidad el efecto de la profundidad de la red y la tasa de aprendizaje, y su interacción. Por último, hemos visto que la inicialización es importante y por qué necesitamos formas inteligentes de inicialización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kaZeoPS73H41"
      },
      "source": [
        "---\n",
        "# Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "q5Rqhcg33H41"
      },
      "source": [
        "## Interacción de hiperparámetros\n",
        "\n",
        "Por último, juntemos todo lo que hemos aprendido y encontremos los mejores pesos iniciales y la tasa de aprendizaje para una profundidad determinada. A estas alturas deberías haber aprendido las interacciones y saber cómo encontrar los valores óptimos rápidamente. Si recibe advertencias de \"desbordamiento numérico\", no se desanime. A menudo son causados por gradientes que \"explotan\" o \"desaparecen\".\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "¿Ha experimentado algún comportamiento sorprendente \n",
        "o dificultad para encontrar los parámetros óptimos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "IkqQ9zZ93H41"
      },
      "outputs": [],
      "source": [
        "# @markdown Asegúrate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(depth_lr_init_interplay,\n",
        "             depth = IntSlider(min=10, max=51, step=5, value=25,\n",
        "                               continuous_update=False),\n",
        "             lr = FloatSlider(min=0.001, max=0.1,\n",
        "                              step=0.005, value=0.005,\n",
        "                              continuous_update=False,\n",
        "                              readout_format='.3f',\n",
        "                              description='eta'),\n",
        "             init_weights = FloatSlider(min=0.1, max=3.0,\n",
        "                                        step=0.1, value=0.9,\n",
        "                                        continuous_update=False,\n",
        "                                        readout_format='.3f',\n",
        "                                        description='initial weights'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2W38BHtHKUNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3.2.Modelos lineales y autodiferenciacion.ipynb",
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}