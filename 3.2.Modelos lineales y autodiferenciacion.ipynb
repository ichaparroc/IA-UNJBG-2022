{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JtK9KJCP3H36"
      },
      "source": [
        "# Inteligencia Artificial - ESIS - UNJBG\n",
        "## Semana 3: Modelos Lineales y Autodiferenciaci贸n 2/3\n",
        "### Docente: MSc.(c) Israel N. Chaparro-Cruz\n",
        "**Basado en: Week 1, Day 2: Linear Deep Learning, By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Saeed Salehi, Andrew Saxe\n",
        "\n",
        "__Content reviewers:__ Polina Turishcheva, Antoine De Comite, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Anoop Kulkarni\n",
        "\n",
        "__Production editors:__ Khalid Almubarak, Gagana B, Spiros Chavlis\n",
        "\n",
        "__Content Traduction:__ Israel N. Chaparro-Cruz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "N2FLn7dg3H4F"
      },
      "source": [
        "---\n",
        "# Objetivos del tutorial\n",
        "\n",
        "* Panorama acerca del entrenamiento\n",
        "* El efecto de la profundidad\n",
        "* Elecci贸n de la tasa de aprendizaje\n",
        "* La inicializaci贸n es importante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "om7_1Z603H4J"
      },
      "source": [
        "---\n",
        "# Configuraci贸n\n",
        "\n",
        "Este es un tutorial sin GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "2QxthTme3H4M"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "oAmGmZnV3H4N"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "\n",
        "from ipywidgets import interact, IntSlider, FloatSlider, fixed\n",
        "from ipywidgets import HBox, interactive_output, ToggleButton, Layout\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "6isOn6303H4P"
      },
      "outputs": [],
      "source": [
        "# @title Plotting functions\n",
        "\n",
        "def plot_x_y_(x_t_, y_t_, x_ev_, y_ev_, loss_log_, weight_log_):\n",
        "  \"\"\"\n",
        "  Plot train data and test results\n",
        "\n",
        "  Args:\n",
        "  x_t_: np.ndarray\n",
        "    Training dataset\n",
        "  y_t_: np.ndarray\n",
        "    Ground truth corresponding to training dataset\n",
        "  x_ev_: np.ndarray\n",
        "    Evaluation set\n",
        "  y_ev_: np.ndarray\n",
        "    ShallowNarrowNet predictions\n",
        "  loss_log_: list\n",
        "    Training loss records\n",
        "  weight_log_: list\n",
        "    Training weight records (evolution of weights)\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.scatter(x_t_, y_t_, c='r', label='training data')\n",
        "  plt.plot(x_ev_, y_ev_, c='b', label='test results', linewidth=2)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.plot(loss_log_, c='r')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('mean squared error')\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.plot(weight_log_)\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('weights')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_vector_field(what, init_weights=None):\n",
        "  \"\"\"\n",
        "  Helper function to plot vector fields\n",
        "\n",
        "  Args:\n",
        "    what: string\n",
        "      If \"all\", plot vectors, trajectories and loss function\n",
        "      If \"vectors\", plot vectors\n",
        "      If \"trajectory\", plot trajectories\n",
        "      If \"loss\", plot loss function\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  n_epochs=40\n",
        "  lr=0.15\n",
        "  x_pos = np.linspace(2.0, 0.5, 100, endpoint=True)\n",
        "  y_pos = 1. / x_pos\n",
        "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
        "  zz = np.empty_like(xx)\n",
        "  x, y = xx[:, 0], yy[0]\n",
        "\n",
        "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
        "\n",
        "  cmap = matplotlib.cm.plasma\n",
        "  plt.figure(figsize=(8, 7))\n",
        "  ax = plt.gca()\n",
        "\n",
        "  if what == 'all' or what == 'vectors':\n",
        "    for i, a in enumerate(x):\n",
        "      for j, b in enumerate(y):\n",
        "        temp_model = ShallowNarrowLNN([a, b])\n",
        "        da, db = temp_model.dloss_dw(x_temp, y_temp)\n",
        "        zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
        "        scale = min(40 * np.sqrt(da**2 + db**2), 50)\n",
        "        ax.quiver(a, b, - da, - db, scale=scale, color=cmap(np.sqrt(da**2 + db**2)))\n",
        "\n",
        "  if what == 'all' or what == 'trajectory':\n",
        "    if init_weights is None:\n",
        "      for init_weights in [[0.5, -0.5], [0.55, -0.45], [-1.8, 1.7]]:\n",
        "        temp_model = ShallowNarrowLNN(init_weights)\n",
        "        _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
        "        ax.scatter(temp_records[:, 0], temp_records[:, 1],\n",
        "                    c=np.arange(len(temp_records)), cmap='Greys')\n",
        "        ax.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
        "        ax.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
        "    else:\n",
        "      temp_model = ShallowNarrowLNN(init_weights)\n",
        "      _, temp_records = temp_model.train(x_temp, y_temp, lr, n_epochs)\n",
        "      ax.scatter(temp_records[:, 0], temp_records[:, 1],\n",
        "                  c=np.arange(len(temp_records)), cmap='Greys')\n",
        "      ax.scatter(temp_records[0, 0], temp_records[0, 1], c='blue', zorder=9)\n",
        "      ax.scatter(temp_records[-1, 0], temp_records[-1, 1], c='red', marker='X', s=100, zorder=9)\n",
        "\n",
        "  if what == 'all' or what == 'loss':\n",
        "    contplt = ax.contourf(x, y, np.log(zz+0.001), zorder=-1, cmap='coolwarm', levels=100)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(contplt, cax=cax)\n",
        "    cbar.set_label('log (Loss)')\n",
        "\n",
        "  ax.set_xlabel(\"$w_1$\")\n",
        "  ax.set_ylabel(\"$w_2$\")\n",
        "  ax.set_xlim(-1.9, 1.9)\n",
        "  ax.set_ylim(-1.9, 1.9)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_landscape():\n",
        "  \"\"\"\n",
        "  Helper function to plot loss landscapes\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  x_temp, y_temp = gen_samples(10, 1.0, 0.0)\n",
        "\n",
        "  xx, yy = np.mgrid[-1.9:2.0:0.2, -1.9:2.0:0.2]\n",
        "  zz = np.empty_like(xx)\n",
        "  x, y = xx[:, 0], yy[0]\n",
        "\n",
        "  for i, a in enumerate(x):\n",
        "    for j, b in enumerate(y):\n",
        "      temp_model = ShallowNarrowLNN([a, b])\n",
        "      zz[i, j] = temp_model.loss(temp_model.forward(x_temp), y_temp)\n",
        "\n",
        "  temp_model = ShallowNarrowLNN([-1.8, 1.7])\n",
        "  loss_rec_1, w_rec_1 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
        "\n",
        "  temp_model = ShallowNarrowLNN([1.5, -1.5])\n",
        "  loss_rec_2, w_rec_2 = temp_model.train(x_temp, y_temp, 0.02, 240)\n",
        "\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  ax = plt.subplot(1, 1, 1, projection='3d')\n",
        "  ax.plot_surface(xx, yy, np.log(zz+0.5), cmap='coolwarm', alpha=0.5)\n",
        "  ax.scatter3D(w_rec_1[:, 0], w_rec_1[:, 1], np.log(loss_rec_1+0.5),\n",
        "                c='k', s=50, zorder=9)\n",
        "  ax.scatter3D(w_rec_2[:, 0], w_rec_2[:, 1], np.log(loss_rec_2+0.5),\n",
        "                c='k', s=50, zorder=9)\n",
        "  plt.axis(\"off\")\n",
        "  ax.view_init(45, 260)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def depth_widget(depth):\n",
        "  \"\"\"\n",
        "  Simulate parameter in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  if depth == 0:\n",
        "    depth_lr_init_interplay(depth, 0.02, 0.9)\n",
        "  else:\n",
        "    depth_lr_init_interplay(depth, 0.01, 0.9)\n",
        "\n",
        "\n",
        "def lr_widget(lr):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth_lr_init_interplay(50, lr, 0.9)\n",
        "\n",
        "\n",
        "def depth_lr_interplay(depth, lr):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth_lr_init_interplay(depth, lr, 0.9)\n",
        "\n",
        "\n",
        "def depth_lr_init_interplay(depth, lr, init_weights):\n",
        "  \"\"\"\n",
        "  Simulate parameters in widget\n",
        "  exploring impact of depth on the training curve\n",
        "  (loss evolution) of a deep but narrow neural network.\n",
        "\n",
        "  Args:\n",
        "    depth: int\n",
        "      Specifies depth of network\n",
        "    lr: float\n",
        "      Specifies learning rate within network\n",
        "    init_weights: list\n",
        "      Specifies initial weights of the network\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  n_epochs = 600\n",
        "\n",
        "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
        "  model = DeepNarrowLNN(np.full((1, depth+1), init_weights))\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
        "            linewidth=3.0, c='m')\n",
        "\n",
        "  plt.title(\"Training a {}-layer LNN with\"\n",
        "  \" $\\eta=${} initialized with $w_i=${}\".format(depth, lr, init_weights), pad=15)\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('Log mean squared error')\n",
        "  plt.ylim(0.001, 1.0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_init_effect():\n",
        "  \"\"\"\n",
        "  Helper function to plot evolution of log mean\n",
        "  squared error over epochs\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  depth = 15\n",
        "  n_epochs = 250\n",
        "  lr = 0.02\n",
        "\n",
        "  x_train, y_train = gen_samples(100, 2.0, 0.1)\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  for init_w in np.arange(0.7, 1.09, 0.05):\n",
        "      model = DeepNarrowLNN(np.full((1, depth), init_w))\n",
        "      plt.plot(model.train(x_train, y_train, lr, n_epochs),\n",
        "              linewidth=3.0, label=\"initial weights {:.2f}\".format(init_w))\n",
        "  plt.title(\"Training a {}-layer narrow LNN with $\\eta=${}\".format(depth, lr), pad=15)\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('Log mean squared error')\n",
        "  plt.legend(loc='lower left', ncol=4)\n",
        "  plt.ylim(0.001, 1.0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "class InterPlay:\n",
        "  \"\"\"\n",
        "  Class specifying parameters for widget\n",
        "  exploring relationship between the depth\n",
        "  and optimal learning rate\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize parameters for InterPlay\n",
        "\n",
        "    Args:\n",
        "      None\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    self.lr = [None]\n",
        "    self.depth = [None]\n",
        "    self.success = [None]\n",
        "    self.min_depth, self.max_depth = 5, 65\n",
        "    self.depth_list = np.arange(10, 61, 10)\n",
        "    self.i_depth = 0\n",
        "    self.min_lr, self.max_lr = 0.001, 0.105\n",
        "    self.n_epochs = 600\n",
        "    self.x_train, self.y_train = gen_samples(100, 2.0, 0.1)\n",
        "    self.converged = False\n",
        "    self.button = None\n",
        "    self.slider = None\n",
        "\n",
        "  def train(self, lr, update=False, init_weights=0.9):\n",
        "    \"\"\"\n",
        "    Train network associated with InterPlay\n",
        "\n",
        "    Args:\n",
        "      lr: float\n",
        "        Specifies learning rate within network\n",
        "      init_weights: float\n",
        "        Specifies initial weights of the network [default: 0.9]\n",
        "      update: boolean\n",
        "        If true, show updates on widget\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    if update and self.converged and self.i_depth < len(self.depth_list):\n",
        "      depth = self.depth_list[self.i_depth]\n",
        "      self.plot(depth, lr)\n",
        "      self.i_depth += 1\n",
        "      self.lr.append(None)\n",
        "      self.depth.append(None)\n",
        "      self.success.append(None)\n",
        "      self.converged = False\n",
        "      self.slider.value = 0.005\n",
        "      if self.i_depth < len(self.depth_list):\n",
        "        self.button.value = False\n",
        "        self.button.description = 'Explore!'\n",
        "        self.button.disabled = True\n",
        "        self.button.button_style = 'Danger'\n",
        "      else:\n",
        "        self.button.value = False\n",
        "        self.button.button_style = ''\n",
        "        self.button.disabled = True\n",
        "        self.button.description = 'Done!'\n",
        "      time.sleep(1.0)\n",
        "\n",
        "    elif self.i_depth < len(self.depth_list):\n",
        "      depth = self.depth_list[self.i_depth]\n",
        "      # Additional assert: self.min_depth <= depth <= self.max_depth\n",
        "      assert self.min_lr <= lr <= self.max_lr\n",
        "      self.converged = False\n",
        "\n",
        "      model = DeepNarrowLNN(np.full((1, depth), init_weights))\n",
        "      self.losses = np.array(model.train(self.x_train, self.y_train, lr, self.n_epochs))\n",
        "      if np.any(self.losses < 1e-2):\n",
        "        success = np.argwhere(self.losses < 1e-2)[0][0]\n",
        "        if np.all((self.losses[success:] < 1e-2)):\n",
        "          self.converged = True\n",
        "          self.success[-1] = success\n",
        "          self.lr[-1] = lr\n",
        "          self.depth[-1] = depth\n",
        "          self.button.disabled = False\n",
        "          self.button.button_style = 'Success'\n",
        "          self.button.description = 'Register!'\n",
        "        else:\n",
        "          self.button.disabled = True\n",
        "          self.button.button_style = 'Danger'\n",
        "          self.button.description = 'Explore!'\n",
        "      else:\n",
        "        self.button.disabled = True\n",
        "        self.button.button_style = 'Danger'\n",
        "        self.button.description = 'Explore!'\n",
        "      self.plot(depth, lr)\n",
        "\n",
        "  def plot(self, depth, lr):\n",
        "    \"\"\"\n",
        "    Plot following subplots:\n",
        "    a. Log mean squared error vs Epochs\n",
        "    b. Learning time vs Depth\n",
        "    c. Optimal learning rate vs Depth\n",
        "\n",
        "    Args:\n",
        "      depth: int\n",
        "        Specifies depth of network\n",
        "      lr: float\n",
        "        Specifies learning rate of network\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    fig = plt.figure(constrained_layout=False, figsize=(10, 8))\n",
        "    gs = fig.add_gridspec(2, 2)\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "    ax1.plot(self.losses, linewidth=3.0, c='m')\n",
        "    ax1.set_title(\"Training a {}-layer LNN with\"\n",
        "    \" $\\eta=${}\".format(depth, lr), pad=15, fontsize=16)\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.set_xlabel('epochs')\n",
        "    ax1.set_ylabel('Log mean squared error')\n",
        "    ax1.set_ylim(0.001, 1.0)\n",
        "\n",
        "    ax2.set_xlim(self.min_depth, self.max_depth)\n",
        "    ax2.set_ylim(-10, self.n_epochs)\n",
        "    ax2.set_xlabel('Depth')\n",
        "    ax2.set_ylabel('Learning time (Epochs)')\n",
        "    ax2.set_title(\"Learning time vs depth\", fontsize=14)\n",
        "    ax2.scatter(np.array(self.depth), np.array(self.success), c='r')\n",
        "\n",
        "    ax3.set_xlim(self.min_depth, self.max_depth)\n",
        "    ax3.set_ylim(self.min_lr, self.max_lr)\n",
        "    ax3.set_xlabel('Depth')\n",
        "    ax3.set_ylabel('Optimal learning rate')\n",
        "    ax3.set_title(\"Empirically optimal $\\eta$ vs depth\", fontsize=14)\n",
        "    ax3.scatter(np.array(self.depth), np.array(self.lr), c='r')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "o4CaZ0fI3H4Z"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "\n",
        "def gen_samples(n, a, sigma):\n",
        "  \"\"\"\n",
        "  Generates n samples with\n",
        "  `y = z * x + noise(sigma)` linear relation.\n",
        "\n",
        "  Args:\n",
        "    n : int\n",
        "      Number of datapoints within sample\n",
        "    a : float\n",
        "      Offset of x\n",
        "    sigma : float\n",
        "      Standard deviation of distribution\n",
        "\n",
        "  Returns:\n",
        "    x : np.array\n",
        "      if sigma > 0, x = random values\n",
        "      else, x = evenly spaced numbers over a specified interval.\n",
        "    y : np.array\n",
        "      y = z * x + noise(sigma)\n",
        "  \"\"\"\n",
        "  assert n > 0\n",
        "  assert sigma >= 0\n",
        "\n",
        "  if sigma > 0:\n",
        "    x = np.random.rand(n)\n",
        "    noise = np.random.normal(scale=sigma, size=(n))\n",
        "    y = a * x + noise\n",
        "  else:\n",
        "    x = np.linspace(0.0, 1.0, n, endpoint=True)\n",
        "    y = a * x\n",
        "  return x, y\n",
        "\n",
        "\n",
        "class ShallowNarrowLNN:\n",
        "  \"\"\"\n",
        "  Shallow and narrow (one neuron per layer)\n",
        "  linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_ws):\n",
        "    \"\"\"\n",
        "    Initialize parameters of ShallowNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      init_ws: initial weights as a list\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    assert isinstance(init_ws, list)\n",
        "    assert len(init_ws) == 2\n",
        "    self.w1 = init_ws[0]\n",
        "    self.w2 = init_ws[1]\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward pass through network y = x * w1 * w2\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Input data\n",
        "\n",
        "    Returns:\n",
        "      y: np.ndarray\n",
        "        y = x * w1 * w2\n",
        "    \"\"\"\n",
        "    y = x * self.w1 * self.w2\n",
        "    return y\n",
        "\n",
        "  def loss(self, y_p, y_t):\n",
        "    \"\"\"\n",
        "    Mean squared error (L2)\n",
        "    with 1/2 for convenience\n",
        "\n",
        "    Args:\n",
        "      y_p: np.ndarray\n",
        "        Network Predictions\n",
        "      y_t: np.ndarray\n",
        "        Targets\n",
        "\n",
        "    Returns:\n",
        "      mse: float\n",
        "        Average mean squared error\n",
        "    \"\"\"\n",
        "    assert y_p.shape == y_t.shape\n",
        "    mse = ((y_t - y_p)**2).mean()\n",
        "    return mse\n",
        "\n",
        "  def dloss_dw(self, x, y_t):\n",
        "    \"\"\"\n",
        "    Partial derivative of loss with respect to weights\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Dataset\n",
        "      y_t : np.array\n",
        "        Corresponding Ground Truth\n",
        "\n",
        "    Returns:\n",
        "      dloss_dw1: float\n",
        "        -mean(2 * self.w2 * x * Error)\n",
        "      dloss_dw2: float\n",
        "        -mean(2 * self.w1 * x * Error)\n",
        "    \"\"\"\n",
        "    assert x.shape == y_t.shape\n",
        "    Error = y_t - self.w1 * self.w2 * x\n",
        "    dloss_dw1 = - (2 * self.w2 * x * Error).mean()\n",
        "    dloss_dw2 = - (2 * self.w1 * x * Error).mean()\n",
        "    return dloss_dw1, dloss_dw2\n",
        "\n",
        "  def train(self, x, y_t, eta, n_ep):\n",
        "    \"\"\"\n",
        "    Gradient descent algorithm\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Dataset\n",
        "      y_t : np.array\n",
        "        Corrsponding target\n",
        "      eta: float\n",
        "        Learning rate\n",
        "      n_ep : int\n",
        "        Number of epochs\n",
        "\n",
        "    Returns:\n",
        "      loss_records: np.ndarray\n",
        "        Log of loss per epoch\n",
        "      weight_records: np.ndarray\n",
        "        Log of weights per epoch\n",
        "    \"\"\"\n",
        "    assert x.shape == y_t.shape\n",
        "\n",
        "    loss_records = np.empty(n_ep)  # Pre allocation of loss records\n",
        "    weight_records = np.empty((n_ep, 2))  # Pre allocation of weight records\n",
        "\n",
        "    for i in range(n_ep):\n",
        "      y_p = self.forward(x)\n",
        "      loss_records[i] = self.loss(y_p, y_t)\n",
        "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_t)\n",
        "      self.w1 -= eta * dloss_dw1\n",
        "      self.w2 -= eta * dloss_dw2\n",
        "      weight_records[i] = [self.w1, self.w2]\n",
        "\n",
        "    return loss_records, weight_records\n",
        "\n",
        "\n",
        "class DeepNarrowLNN:\n",
        "  \"\"\"\n",
        "  Deep but thin (one neuron per layer)\n",
        "  linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_ws):\n",
        "    \"\"\"\n",
        "    Initialize parameters of DeepNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      init_ws: np.ndarray\n",
        "        Initial weights as a numpy array\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    self.n = init_ws.size\n",
        "    self.W = init_ws.reshape(1, -1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of DeepNarrowLNN\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      y: np.array\n",
        "        Product of weights over input features\n",
        "    \"\"\"\n",
        "    y = np.prod(self.W) * x\n",
        "    return y\n",
        "\n",
        "  def loss(self, y_t, y_p):\n",
        "    \"\"\"\n",
        "    Mean squared error (L2 loss)\n",
        "\n",
        "    Args:\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      y_p : np.array\n",
        "        Network's predictions\n",
        "\n",
        "    Returns:\n",
        "      mse: float\n",
        "        Mean squared error\n",
        "    \"\"\"\n",
        "    assert y_p.shape == y_t.shape\n",
        "    mse = ((y_t - y_p)**2 / 2).mean()\n",
        "    return mse\n",
        "\n",
        "  def dloss_dw(self, x, y_t, y_p):\n",
        "    \"\"\"\n",
        "    Analytical gradient of weights\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input features\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      y_p : np.array\n",
        "        Network Predictions\n",
        "\n",
        "    Returns:\n",
        "      dW: np.ndarray\n",
        "        Analytical gradient of weights\n",
        "    \"\"\"\n",
        "    E = y_t - y_p  # i.e., y_t - x * np.prod(self.W)\n",
        "    Ex = np.multiply(x, E).mean()\n",
        "    Wp = np.prod(self.W) / (self.W + 1e-9)\n",
        "    dW = - Ex * Wp\n",
        "    return dW\n",
        "\n",
        "  def train(self, x, y_t, eta, n_epochs):\n",
        "    \"\"\"\n",
        "    Training using gradient descent\n",
        "\n",
        "    Args:\n",
        "      x : np.array\n",
        "        Input Features\n",
        "      y_t : np.array\n",
        "        Targets\n",
        "      eta: float\n",
        "        Learning rate\n",
        "      n_epochs : int\n",
        "        Number of epochs\n",
        "\n",
        "    Returns:\n",
        "      loss_records: np.ndarray\n",
        "        Log of loss over epochs\n",
        "    \"\"\"\n",
        "    loss_records = np.empty(n_epochs)\n",
        "    loss_records[:] = np.nan\n",
        "    for i in range(n_epochs):\n",
        "      y_p = self.forward(x)\n",
        "      loss_records[i] = self.loss(y_t, y_p).mean()\n",
        "      dloss_dw = self.dloss_dw(x, y_t, y_p)\n",
        "      if np.isnan(dloss_dw).any() or np.isinf(dloss_dw).any():\n",
        "        return loss_records\n",
        "      self.W -= eta * dloss_dw\n",
        "    return loss_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "6aj5KEJY3H4c"
      },
      "outputs": [],
      "source": [
        "#@title Set random seed\n",
        "\n",
        "#@markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# For DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "GbjdOftY3H4e"
      },
      "outputs": [],
      "source": [
        "#@title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# Inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"GPU is not enabled in this notebook. \\n\"\n",
        "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook. \\n\"\n",
        "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "mDM4c91h3H4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414579f0-ea6d-46b4-e11f-1d9b2ce3d040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is not enabled in this notebook. \n",
            "If you want to enable it, in the menu under `Runtime` -> \n",
            "`Hardware accelerator.` and select `GPU` from the dropdown menu\n"
          ]
        }
      ],
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gQro91xA3H4h"
      },
      "source": [
        "---\n",
        "# Secci贸n 1: Una red neuronal lineal poco profunda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ghmk6_bA3H4i"
      },
      "source": [
        "## Secci贸n 1.1: Una red lineal poco profunda y estrecha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aFbJ4b7B3H4j"
      },
      "source": [
        "Para entender mejor el comportamiento del entrenamiento de redes neuronales con descenso de gradiente, empezamos con el caso incre铆blemente sencillo de una red neuronal lineal estrecha y poco profunda, ya que los modelos m谩s avanzados son imposibles de diseccionar y comprender con nuestras herramientas matem谩ticas actuales.\n",
        "\n",
        "El modelo que utilizamos tiene una capa oculta, con una sola neurona, y dos pesos. Consideramos el error al cuadrado (o p茅rdida L2) como funci贸n de coste. Como ya habr谩 adivinado, podemos visualizar el modelo como una red neuronal:\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/shallow_narrow_nn.png\" width=\"400\"/></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "o por su gr谩fico de c谩lculo:\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/shallow_narrow.png\" alt=\"Shallow Narrow Graph\" width=\"400\"/></center>\n",
        "\n",
        "o, en raras ocasiones, incluso como una cartograf铆a razonablemente compacta:\n",
        "\n",
        "$$ loss = (y - w_1 \\cdot w_2 \\cdot x)^2 $$\n",
        "\n",
        "<br/>\n",
        "\n",
        "Implementar una red neuronal desde cero sin utilizar ninguna herramienta de diferenciaci贸n autom谩tica es raramente necesario. Por lo tanto, los dos siguientes ejercicios son **Bonus** (opcionales). Por favor, ign贸relos si tiene alg煤n l铆mite de tiempo o presi贸n y contin煤e con la Secci贸n 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TNhkkPDp3H4j"
      },
      "source": [
        "### Ejercicio anal铆tico 1.1: Gradientes de p茅rdidas (opcional)\n",
        "\n",
        "Una vez m谩s, le pedimos que calcule los gradientes de la red de forma anal铆tica, ya que los necesitar谩 para el siguiente ejercicio. Entendemos lo molesto que es esto.\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_1}} = ?$\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_2}} = ?$\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "#### Soluci贸n\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_1}} = -2 \\cdot w_2 \\cdot x \\cdot (y - w_1 \\cdot w_2 \\cdot x)$\n",
        "\n",
        "$\\dfrac{\\partial{loss}}{\\partial{w_2}} = -2 \\cdot w_1 \\cdot x \\cdot (y - w_1 \\cdot w_2 \\cdot x)$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EdlDxyC63H4k"
      },
      "source": [
        "### Ejercicio de codificaci贸n 1.1: Implementar una LNN estrecha simple (Opcional)\n",
        "\n",
        "A continuaci贸n, te pedimos que implementes el pase `forward` para nuestro modelo desde cero sin usar PyTorch.\n",
        "\n",
        "Adem谩s, aunque nuestro modelo obtiene una 煤nica caracter铆stica de entrada y emite una 煤nica predicci贸n, podr铆amos calcular la p茅rdida y realizar el entrenamiento para m煤ltiples muestras a la vez. Esta es la pr谩ctica com煤n para las redes neuronales, ya que los ordenadores son incre铆blemente r谩pidos haciendo operaciones matriciales (o tensoriales) en lotes de datos, en lugar de procesar las muestras de una en una a trav茅s de bucles `for`. Por lo tanto, para la funci贸n \"p茅rdida\", por favor, implemente el error cuadr谩tico medio (MSE), y ajuste sus gradientes anal铆ticos en consecuencia cuando implemente la funci贸n \"dloss_dw\".\n",
        "\n",
        "Por 煤ltimo, completar la funci贸n `train` para el algoritmo de descenso de gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla loss (\\mathbf{w}^{(t)})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "aueV78_H3H4k"
      },
      "outputs": [],
      "source": [
        "class ShallowNarrowExercise:\n",
        "  \"\"\"\n",
        "  Shallow and narrow (one neuron per layer) linear neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_weights):\n",
        "    \"\"\"\n",
        "    Initialize parameters of ShallowNarrow Net\n",
        "\n",
        "    Args:\n",
        "      init_weights: list\n",
        "        Initial weights\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    assert isinstance(init_weights, (list, np.ndarray, tuple))\n",
        "    assert len(init_weights) == 2\n",
        "    self.w1 = init_weights[0]\n",
        "    self.w2 = init_weights[1]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward pass through netwrok y = x * w1 * w2\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "\n",
        "    Returns:\n",
        "      y: np.ndarray\n",
        "        Neural network output (predictions)\n",
        "    \"\"\"\n",
        "    #################################################\n",
        "    ## Implement the forward pass to calculate prediction\n",
        "    ## Note that prediction is not the loss\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
        "    #################################################\n",
        "    y = ...\n",
        "    return y\n",
        "\n",
        "\n",
        "  def dloss_dw(self, x, y_true):\n",
        "    \"\"\"\n",
        "    Gradient of loss with respect to weights\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "      y_true: np.ndarray\n",
        "        True labels\n",
        "\n",
        "    Returns:\n",
        "      dloss_dw1: float\n",
        "        Mean gradient of loss with respect to w1\n",
        "      dloss_dw2: float\n",
        "        Mean gradient of loss with respect to w2\n",
        "    \"\"\"\n",
        "    assert x.shape == y_true.shape\n",
        "    #################################################\n",
        "    ## Implement the gradient computation function\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Gradient of Loss `dloss_dw`\")\n",
        "    #################################################\n",
        "    dloss_dw1 = ...\n",
        "    dloss_dw2 = ...\n",
        "    return dloss_dw1, dloss_dw2\n",
        "\n",
        "\n",
        "  def train(self, x, y_true, lr, n_ep):\n",
        "    \"\"\"\n",
        "    Training with Gradient descent algorithm\n",
        "\n",
        "    Args:\n",
        "      x: np.ndarray\n",
        "        Features (inputs) to neural net\n",
        "      y_true: np.ndarray\n",
        "        True labels\n",
        "      lr: float\n",
        "        Learning rate\n",
        "      n_ep: int\n",
        "        Number of epochs (training iterations)\n",
        "\n",
        "    Returns:\n",
        "      loss_records: list\n",
        "        Training loss records\n",
        "      weight_records: list\n",
        "        Training weight records (evolution of weights)\n",
        "    \"\"\"\n",
        "    assert x.shape == y_true.shape\n",
        "\n",
        "    loss_records = np.empty(n_ep)  # Pre allocation of loss records\n",
        "    weight_records = np.empty((n_ep, 2))  # Pre allocation of weight records\n",
        "\n",
        "    for i in range(n_ep):\n",
        "      y_prediction = self.forward(x)\n",
        "      loss_records[i] = loss(y_prediction, y_true)\n",
        "      dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_true)\n",
        "      #################################################\n",
        "      ## Implement the gradient descent step\n",
        "      # Complete the function and remove or comment the line below\n",
        "      raise NotImplementedError(\"Training loop `train`\")\n",
        "      #################################################\n",
        "      self.w1 -= ...\n",
        "      self.w2 -= ...\n",
        "      weight_records[i] = [self.w1, self.w2]\n",
        "\n",
        "    return loss_records, weight_records\n",
        "\n",
        "\n",
        "def loss(y_prediction, y_true):\n",
        "  \"\"\"\n",
        "  Mean squared error\n",
        "\n",
        "  Args:\n",
        "    y_prediction: np.ndarray\n",
        "      Model output (prediction)\n",
        "    y_true: np.ndarray\n",
        "      True label\n",
        "\n",
        "  Returns:\n",
        "    mse: np.ndarray\n",
        "      Mean squared error loss\n",
        "  \"\"\"\n",
        "  assert y_prediction.shape == y_true.shape\n",
        "  #################################################\n",
        "  ## Implement the MEAN squared error\n",
        "  # Complete the function and remove or comment the line below\n",
        "  raise NotImplementedError(\"Loss function `loss`\")\n",
        "  #################################################\n",
        "  mse = ...\n",
        "  return mse\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "n_epochs = 211\n",
        "learning_rate = 0.02\n",
        "initial_weights = [1.4, -1.6]\n",
        "x_train, y_train = gen_samples(n=73, a=2.0, sigma=0.2)\n",
        "x_eval = np.linspace(0.0, 1.0, 37, endpoint=True)\n",
        "## Uncomment to run\n",
        "# sn_model = ShallowNarrowExercise(initial_weights)\n",
        "# loss_log, weight_log = sn_model.train(x_train, y_train, learning_rate, n_epochs)\n",
        "# y_eval = sn_model.forward(x_eval)\n",
        "# plot_x_y_(x_train, y_train, x_eval, y_eval, loss_log, weight_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "7GPWuV-p3H4l"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial2_Solution_b008d048.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1696.0 height=544.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/W1D2_Tutorial2_Solution_b008d048_1.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "o4ohXAXE3H4m"
      },
      "source": [
        "## Secci贸n 1.2: Panorama de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8r0zhglE3H4n"
      },
      "source": [
        "Como ya te habr谩s preguntado, podemos encontrar anal铆ticamente $w_1$ y $w_2$ sin utilizar el descenso de gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "w_1 \\cdot w_2 = \\dfrac{y}{x}\n",
        "\\end{equation}\n",
        "\n",
        "De hecho, podemos representar los gradientes, la funci贸n de p茅rdida y todas las soluciones posibles en una sola figura. En este ejemplo, utilizamos la cartograf铆a $y = 1x$:\n",
        "\n",
        "**Cinta azul**: muestra todas las soluciones posibles: $~ w_1 w_2 = \\dfrac{y}{x} = \\dfrac{x}{x} = 1 \\Rightarrow w_1 = \\dfrac{1}{w_2}$\n",
        "\n",
        "**Fondo del contorno**: Muestra los valores de p茅rdida, siendo el rojo una p茅rdida mayor\n",
        "\n",
        "**Campo vectorial (flechas)**: muestra el campo vectorial del gradiente. Las flechas amarillas m谩s grandes muestran gradientes m谩s grandes, que corresponden a pasos m谩s grandes por el descenso de gradiente.\n",
        "\n",
        "**C铆rculos de dispersi贸n**: la trayectoria (evoluci贸n) de los pesos durante el entrenamiento para tres inicializaciones diferentes, con puntos azules marcando el inicio del entrenamiento y cruces rojas ( **x** ) marcando el final del entrenamiento. Tambi茅n puede probar sus propias inicializaciones (mantenga los valores iniciales entre `-2,0` y `2,0`) como se muestra aqu铆:\n",
        "\n",
        "```python\n",
        "plot_vector_field('all', [1.0, -1.0])\n",
        "```\n",
        "\n",
        "Por 煤ltimo, si la parcela est谩 demasiado llena, no dude en pasar una de las siguientes cadenas como argumento:\n",
        "\n",
        "```python\n",
        "plot_vector_field('vectors')  # For vector field\n",
        "plot_vector_field('trajectory')  # For training trajectory\n",
        "plot_vector_field('loss')  # For loss contour\n",
        "```\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "Explora los dos gr谩ficos siguientes. Prueba con diferentes valores iniciales.  驴Puedes encontrar el punto de equilibrio? 驴Por qu茅 el entrenamiento se ralentiza cerca de los m铆nimos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "9tXwsU2N3H4n"
      },
      "outputs": [],
      "source": [
        "plot_vector_field('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ygO5qCSW3H4n"
      },
      "source": [
        "Here, we also visualize the loss landscape in a 3-D plot, with two training trajectories for different initial conditions.\n",
        "Note: the trajectories from the 3D plot and the previous plot are independent and different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "UXypGK1g3H4o"
      },
      "outputs": [],
      "source": [
        "plot_loss_landscape()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "h6-_hzYa3H4p"
      },
      "source": [
        "---\n",
        "# Secci贸n 2: Profundidad, tasa de aprendizaje e inicializaci贸n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "FDgA-vtI3H4p"
      },
      "source": [
        "Los modelos de aprendizaje profundo exitosos suelen ser desarrollados por un equipo de personas muy inteligentes, que pasan muchas horas \"ajustando\" los hiperpar谩metros de aprendizaje y encontrando inicializaciones eficaces. En esta secci贸n, examinamos tres hiperpar谩metros b谩sicos (pero a menudo no simples): la profundidad, la tasa de aprendizaje y la inicializaci贸n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sCSRLYIa3H4q"
      },
      "source": [
        "## Secci贸n 2.1: El efecto de la profundidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iBGJhyyV3H4q"
      },
      "source": [
        "驴Por qu茅 puede ser 煤til la profundidad? 驴Qu茅 hace que una red o sistema de aprendizaje sea \"profundo\"? La realidad es que las redes neuronales poco profundas suelen ser incapaces de aprender funciones complejas debido a las limitaciones de los datos. En cambio, la profundidad parece m谩gica. La profundidad puede cambiar las funciones que una red puede representar, la forma en que una red aprende y c贸mo una red generaliza a datos no vistos. \n",
        "\n",
        "Veamos los retos que plantea la profundidad en el entrenamiento de una red neuronal. Imaginemos una red lineal de una sola entrada y una sola salida con 50 capas ocultas y una sola neurona por capa (es decir, una red neuronal profunda estrecha). La salida de la red es f谩cil de calcular:\n",
        "\n",
        "$$ prediction = x \\cdot w_1 \\cdot w_2 \\cdot \\cdot \\cdot w_{50} $$\n",
        "\n",
        "Si el valor inicial de todos los pesos es $w_i = 2$, la predicci贸n para $x=1$ ser铆a **explosiva**: $y_p = 2^{50} \\approx 1,1256 \\times 10^{15}$. Por otro lado, para pesos inicializados a $w_i = 0,5$, la salida es **desvaneciente**: $y_p = 0,5^{50} \\approx 8,88 \\times 10^{-16}$. Del mismo modo, si recordamos la regla de la cadena, a medida que el gr谩fico se hace m谩s profundo, el n煤mero de elementos en la multiplicaci贸n de la cadena aumenta, lo que podr铆a llevar a la explosi贸n o desaparici贸n de los gradientes. Para evitar estas vulnerabilidades num茅ricas que podr铆an perjudicar a nuestro algoritmo de entrenamiento, tenemos que entender el efecto de la profundidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mRCLyhyJ3H4r"
      },
      "source": [
        "### Demostraci贸n interactiva 2.1: Widget de profundidad\n",
        "\n",
        "Utilice el widget para explorar el impacto de la profundidad en la curva de entrenamiento (evoluci贸n de p茅rdidas) de una red neuronal profunda pero estrecha.\n",
        "\n",
        "**隆Piensa!**\n",
        "\n",
        "驴Qu茅 redes se entrenaron m谩s r谩pido? 驴Todas las redes acabaron por \"funcionar\" (converger)? 驴Cu谩l es la forma de su trayectoria de aprendizaje?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "8lkIS1LM3H4s"
      },
      "outputs": [],
      "source": [
        "# @markdown Aseg煤rate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(depth_widget,\n",
        "    depth = IntSlider(min=0, max=51,\n",
        "                      step=5, value=0,\n",
        "                      continuous_update=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Qi2i2mfi3H4t"
      },
      "source": [
        "## Secci贸n 2.2: Elecci贸n de la tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JAq0oFIj3H4t"
      },
      "source": [
        "La tasa de aprendizaje es un hiperpar谩metro com煤n para la mayor铆a de los algoritmos de optimizaci贸n. 驴C贸mo debemos fijarlo? A veces, la 煤nica opci贸n es probar todas las posibilidades, pero a veces conocer algunas compensaciones clave nos ayudar谩 a guiar nuestra b煤squeda de buenos hiperpar谩metros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "QTeSn1v_3H4u"
      },
      "source": [
        "### Demostraci贸n interactiva 2.2: Widget de tasa de aprendizaje\n",
        "\n",
        "Aqu铆, fijamos la profundidad de la red en 50 capas. Utilice el widget para explorar el impacto de la tasa de aprendizaje $\\eta$ en la curva de entrenamiento (evoluci贸n de las p茅rdidas) de una red neuronal profunda pero estrecha.\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "驴Podemos decir que un mayor ritmo de aprendizaje siempre conduce a un aprendizaje m谩s r谩pido? 驴Por qu茅 no? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "2ZcwOD523H4u"
      },
      "outputs": [],
      "source": [
        "# @markdown Aseg煤rate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(lr_widget,\n",
        "    lr = FloatSlider(min=0.005, max=0.045, step=0.005, value=0.005,\n",
        "                     continuous_update=False, readout_format='.3f',\n",
        "                     description='eta'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DnQWhnP63H4v"
      },
      "source": [
        "## Secci贸n 2.3: Profundidad vs. Ritmo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "owi8q-mx3H4w"
      },
      "source": [
        "### Demostraci贸n Interactiva 2.3: Profundidad y ritmo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-ZzTnf__3H4w"
      },
      "source": [
        "**Instrucci贸n importante**\n",
        "El ejercicio comienza con 10 capas ocultas. Tu tarea es encontrar la tasa de aprendizaje que proporcione una convergencia (aprendizaje) r谩pida pero robusta. Cuando est茅s seguro de la tasa de aprendizaje, puedes **Registrar** la tasa de aprendizaje 贸ptima para la profundidad dada. Una vez que se pulsa el bot贸n de registro, se instala un modelo m谩s profundo, de modo que se puede encontrar la siguiente tasa de aprendizaje 贸ptima. El bot贸n de registro se vuelve verde s贸lo cuando el entrenamiento converge, pero no implica la convergencia m谩s r谩pida. Por 煤ltimo, ten paciencia :) los widgets son lentos.\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "驴Puedes explicar la relaci贸n entre la profundidad y la tasa de aprendizaje 贸ptima?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "_gOwoegq3H4w"
      },
      "outputs": [],
      "source": [
        "# @markdown Aseg煤rate de ejecutar esta celda para habilitar el widget.\n",
        "intpl_obj = InterPlay()\n",
        "\n",
        "intpl_obj.slider = FloatSlider(min=0.005, max=0.105, step=0.005, value=0.005,\n",
        "                               layout=Layout(width='500px'),\n",
        "                               continuous_update=False,\n",
        "                               readout_format='.3f',\n",
        "                               description='eta')\n",
        "\n",
        "intpl_obj.button = ToggleButton(value=intpl_obj.converged, description='Register')\n",
        "\n",
        "widgets_ui = HBox([intpl_obj.slider, intpl_obj.button])\n",
        "widgets_out = interactive_output(intpl_obj.train,\n",
        "                                 {'lr': intpl_obj.slider,\n",
        "                                  'update': intpl_obj.button,\n",
        "                                  'init_weights': fixed(0.9)})\n",
        "\n",
        "display(widgets_ui, widgets_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "B4CflwoV3H4x"
      },
      "source": [
        "## Secci贸n 2.4: Por qu茅 es importante la inicializaci贸n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qRYN9f_J3H4y"
      },
      "source": [
        "Hemos visto, incluso en los casos m谩s sencillos, que la profundidad puede ralentizar el aprendizaje. 驴Por qu茅? Por la regla de la cadena, los gradientes se multiplican por el peso actual en cada capa, por lo que el producto puede desaparecer o explotar. Por lo tanto, la inicializaci贸n de los pesos es un hiperpar谩metro fundamentalmente importante.\n",
        "\n",
        "Aunque en la pr谩ctica los valores iniciales para los par谩metros aprendibles son a menudo muestreados de diferentes $\\mathcal{Uniform}$ o $\\mathcal{Normal}$ distribuci贸n de probabilidad, aqu铆 utilizamos un 煤nico valor para todos los par谩metros.\n",
        "\n",
        "La figura siguiente muestra el efecto de la inicializaci贸n en la velocidad de aprendizaje para la LNN profunda pero estrecha. Hemos excluido las inicializaciones que conducen a errores num茅ricos como `nan` o `inf`, que son la consecuencia de inicializaciones m谩s peque帽as o m谩s grandes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "91wfXMJB3H4z"
      },
      "outputs": [],
      "source": [
        "# @markdown 隆Aseg煤rate de ejecutar esta celda para ver la figura!\n",
        "\n",
        "plot_init_effect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lAzo2bt63H40"
      },
      "source": [
        "---\n",
        "# Resumen\n",
        "\n",
        "En el segundo tutorial, hemos aprendido qu茅 es el paisaje de entrenamiento, y tambi茅n hemos visto en profundidad el efecto de la profundidad de la red y la tasa de aprendizaje, y su interacci贸n. Por 煤ltimo, hemos visto que la inicializaci贸n es importante y por qu茅 necesitamos formas inteligentes de inicializaci贸n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kaZeoPS73H41"
      },
      "source": [
        "---\n",
        "# Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "q5Rqhcg33H41"
      },
      "source": [
        "## Interacci贸n de hiperpar谩metros\n",
        "\n",
        "Por 煤ltimo, juntemos todo lo que hemos aprendido y encontremos los mejores pesos iniciales y la tasa de aprendizaje para una profundidad determinada. A estas alturas deber铆as haber aprendido las interacciones y saber c贸mo encontrar los valores 贸ptimos r谩pidamente. Si recibe advertencias de \"desbordamiento num茅rico\", no se desanime. A menudo son causados por gradientes que \"explotan\" o \"desaparecen\".\n",
        "\n",
        "**Piensa!**\n",
        "\n",
        "驴Ha experimentado alg煤n comportamiento sorprendente \n",
        "o dificultad para encontrar los par谩metros 贸ptimos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "IkqQ9zZ93H41"
      },
      "outputs": [],
      "source": [
        "# @markdown Aseg煤rate de ejecutar esta celda para habilitar el widget.\n",
        "\n",
        "_ = interact(depth_lr_init_interplay,\n",
        "             depth = IntSlider(min=10, max=51, step=5, value=25,\n",
        "                               continuous_update=False),\n",
        "             lr = FloatSlider(min=0.001, max=0.1,\n",
        "                              step=0.005, value=0.005,\n",
        "                              continuous_update=False,\n",
        "                              readout_format='.3f',\n",
        "                              description='eta'),\n",
        "             init_weights = FloatSlider(min=0.1, max=3.0,\n",
        "                                        step=0.1, value=0.9,\n",
        "                                        continuous_update=False,\n",
        "                                        readout_format='.3f',\n",
        "                                        description='initial weights'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2W38BHtHKUNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3.2.Modelos lineales y autodiferenciacion.ipynb",
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}