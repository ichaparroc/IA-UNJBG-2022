{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "wib90P0oeKh3"
      },
      "source": [
        "# Inteligencia Artificial - ESIS - UNJBG\n",
        "## Semana 3: Modelos Lineales y Autodiferenciación 1/3\n",
        "### Docente: MSc.(c) Israel N. Chaparro-Cruz\n",
        "\n",
        "**Basado en: Week 1, Day 2: Linear Deep Learning, By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Saeed Salehi, Vladimir Haltakov, Andrew Saxe\n",
        "\n",
        "__Content reviewers:__ Polina Turishcheva, Antoine De Comite, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Anoop Kulkarni, Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Khalid Almubarak, Spiros Chavlis\n",
        "\n",
        "__Post-Production team:__ Gagana B, Spiros Chavlis\n",
        "\n",
        "__Content Traduction:__ Israel N. Chaparro-Cruz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UUyPlmWIeKh-"
      },
      "source": [
        "---\n",
        "# Objetivos del Tutorial\n",
        "\n",
        "Se continuará con la construcción del conjunto de habilidades de PyTorch y motivará su funcionalidad principal: Autograd. En este cuaderno, cubriremos los conceptos e ideas clave de:\n",
        "\n",
        "* Descenso de gradientes\n",
        "* PyTorch Autograd\n",
        "* Módulo `nn` de PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AeRO9876eKiD"
      },
      "source": [
        "---\n",
        "# Configuración\n",
        "\n",
        "Este es un tutorial sin GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "dynlnFmAeKiF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from math import pi\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "HKeht2OLeKiG"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "import ipywidgets as widgets       # Interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "IbkJL-K6eKiH"
      },
      "outputs": [],
      "source": [
        "# @title Plotting functions\n",
        "\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def ex3_plot(model, x, y, ep, lss):\n",
        "  \"\"\"\n",
        "  Plot training loss\n",
        "\n",
        "  Args:\n",
        "    model: nn.module\n",
        "      Model implementing regression\n",
        "    x: np.ndarray\n",
        "      Training Data\n",
        "    y: np.ndarray\n",
        "      Targets\n",
        "    ep: int\n",
        "      Number of epochs\n",
        "    lss: function\n",
        "      Loss function\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "  ax1.set_title(\"Regression\")\n",
        "  ax1.plot(x, model(x).detach().numpy(), color='r', label='prediction')\n",
        "  ax1.scatter(x, y, c='c', label='targets')\n",
        "  ax1.set_xlabel('x')\n",
        "  ax1.set_ylabel('y')\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2.set_title(\"Training loss\")\n",
        "  ax2.plot(np.linspace(1, epochs, epochs), losses, color='y')\n",
        "  ax2.set_xlabel(\"Epoch\")\n",
        "  ax2.set_ylabel(\"MSE\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def ex1_plot(fun_z, fun_dz):\n",
        "  \"\"\"\n",
        "  Plots the function and gradient vectors\n",
        "\n",
        "  Args:\n",
        "    fun_z: f.__name__\n",
        "      Function implementing sine function\n",
        "    fun_dz: f.__name__\n",
        "      Function implementing sine function as gradient vector\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  x, y = np.arange(-3, 3.01, 0.02), np.arange(-3, 3.01, 0.02)\n",
        "  xx, yy = np.meshgrid(x, y, sparse=True)\n",
        "  zz = fun_z(xx, yy)\n",
        "  xg, yg = np.arange(-2.5, 2.6, 0.5), np.arange(-2.5, 2.6, 0.5)\n",
        "  xxg, yyg = np.meshgrid(xg, yg, sparse=True)\n",
        "  zxg, zyg = fun_dz(xxg, yyg)\n",
        "\n",
        "  plt.figure(figsize=(8, 7))\n",
        "  plt.title(\"Gradient vectors point towards steepest ascent\")\n",
        "  contplt = plt.contourf(x, y, zz, levels=20)\n",
        "  plt.quiver(xxg, yyg, zxg, zyg, scale=50, color='r', )\n",
        "  plt.xlabel('$x$')\n",
        "  plt.ylabel('$y$')\n",
        "  ax = plt.gca()\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  cbar = plt.colorbar(contplt, cax=cax)\n",
        "  cbar.set_label('$z = h(x, y)$')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "oeam0ly5eKiJ"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# For DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "wJgNRjKpeKiK"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"GPU is not enabled in this notebook. \\n\"\n",
        "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook. \\n\"\n",
        "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "tyJr3Hu-eKiL"
      },
      "outputs": [],
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Wi1GAk4xeKiM"
      },
      "source": [
        "---\n",
        "# Sección 0: Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Az5gqPqSeKiM"
      },
      "source": [
        "Vamos a recorrer 3 tutoriales. \n",
        "\n",
        "- Comenzando con el Descenso de Gradientes, el caballo de batalla de los algoritmos de aprendizaje profundo. \n",
        "- El segundo tutorial nos ayudará a construir una mejor intuición sobre las redes neuronales y los hiperparámetros básicos. \n",
        "- Por último, en el tutorial 3, aprenderemos sobre la dinámica de aprendizaje, lo que aprende una (buena) red profunda, y por qué a veces pueden tener un mal rendimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pemTkVn5eKiN"
      },
      "source": [
        "---\n",
        "# Sección 1: Algoritmo de descenso de gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fto56LAgeKiO"
      },
      "source": [
        "Dado que el objetivo de la mayoría de los algoritmos de aprendizaje es **minimizar la función de riesgo (también conocida como coste o pérdida)**, la optimización suele ser el núcleo de la mayoría de las técnicas de aprendizaje automático. El algoritmo de descenso de gradiente, junto con sus variaciones como el descenso de gradiente estocástico, es uno de los métodos de optimización más potentes y populares utilizados para el aprendizaje profundo. Hoy presentaremos los fundamentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ouDcnS0peKiO"
      },
      "source": [
        "## Sección 1.1: Gradientes y ascensos más pronunciados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TYrUMV1jeKiP"
      },
      "source": [
        "Antes de introducir el algoritmo de descenso de gradiente, vamos a repasar una propiedad muy importante de los gradientes. El gradiente de una función siempre apunta en la dirección del ascenso más pronunciado. El siguiente ejercicio nos ayudará a aclarar esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BrYsfxNIeKiQ"
      },
      "source": [
        "### Ejercicio analítico 1.1: Vector gradiente (Opcional)\n",
        "\n",
        "Dada la siguiente función:\n",
        "\n",
        "\\begin{equation}\n",
        "z = h(x, y) = \\sin(x^2 + y^2)\n",
        "\\end{equation}\n",
        "\n",
        "encontrar el vector gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\begin{bmatrix}\n",
        "  \\dfrac{\\partial z}{\\partial x} \\\\ \\\\ \\dfrac{\\partial z}{\\partial y}\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "*Pista: Usa la regla de la cadena*\n",
        "\n",
        "**Regla de la cadena: Para una función compuesta $F(x) = g(h(x)) \\equiv (g \\circ h)(x)$:\n",
        "\n",
        "\\begin{equation}\n",
        "F'(x) = g'(h(x)) \\cdot h'(x)\n",
        "\\end{equation}\n",
        "\n",
        "o con otra denominación:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dF}{dx} = \\frac{dg}{dh} ~ \\frac{dh}{dx}\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "#### Solución:\n",
        "Podemos reescribir la función como una función compuesta:\n",
        "\n",
        "\\begin{equation}\n",
        "z = f\\left( g(x,y) \\right), ~~ f(u) = \\sin(u), ~~ g(x, y) = x^2 + y^2\n",
        "\\end{equation}\n",
        "\n",
        "Usando la [regla de la cadena](https://en.wikipedia.org/wiki/Chain_rule):\n",
        "\n",
        "\\begin{align}\n",
        "\\dfrac{\\partial z}{\\partial x} &= \\dfrac{\\partial f}{\\partial g} \\dfrac{\\partial g}{\\partial x} = \\cos(g(x,y)) ~ (2x) = \\cos(x^2 + y^2) \\cdot 2x \\\\ \\\\\n",
        "\\dfrac{\\partial z}{\\partial y} &= \\dfrac{\\partial f}{\\partial g} \\dfrac{\\partial g}{\\partial y} = \\cos(g(x,y)) ~ (2y) = \\cos(x^2 + y^2) \\cdot 2y\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vVbVJq_ceKiQ"
      },
      "source": [
        "### Ejercicio 1.1: Vector de gradiente\n",
        "\n",
        "Implementa (completa) la función que devuelve el vector gradiente para $z=\\sin(x^2 + y^2)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "5iLzJFAmeKiR"
      },
      "outputs": [],
      "source": [
        "def fun_z(x, y):\n",
        "  \"\"\"\n",
        "  Implements function sin(x^2 + y^2)\n",
        "\n",
        "  Args:\n",
        "    x: (float, np.ndarray)\n",
        "      Variable x\n",
        "    y: (float, np.ndarray)\n",
        "      Variable y\n",
        "\n",
        "  Returns:\n",
        "    z: (float, np.ndarray)\n",
        "      sin(x^2 + y^2)\n",
        "  \"\"\"\n",
        "  z = np.sin(x**2 + y**2)\n",
        "  return z\n",
        "\n",
        "\n",
        "def fun_dz(x, y):\n",
        "  \"\"\"\n",
        "  Implements function sin(x^2 + y^2)\n",
        "\n",
        "  Args:\n",
        "    x: (float, np.ndarray)\n",
        "      Variable x\n",
        "    y: (float, np.ndarray)\n",
        "      Variable y\n",
        "\n",
        "  Returns:\n",
        "    Tuple of gradient vector for sin(x^2 + y^2)\n",
        "  \"\"\"\n",
        "  #################################################\n",
        "  ## Implement the function which returns gradient vector\n",
        "  ## Complete the partial derivatives dz_dx and dz_dy\n",
        "  # Complete the function and remove or comment the line below\n",
        "  raise NotImplementedError(\"Gradient function `fun_dz`\")\n",
        "  #################################################\n",
        "  dz_dx = ...\n",
        "  dz_dy = ...\n",
        "  return (dz_dx, dz_dy)\n",
        "\n",
        "## Uncomment to run\n",
        "# ex1_plot(fun_z, fun_dz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8qb6dizVeKiS"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial1_Solution_115a15ba.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1120.0 height=976.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/W1D2_Tutorial1_Solution_115a15ba_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2raOXO0peKiS"
      },
      "source": [
        "Podemos ver en el gráfico que para cualquier $x_0$ e $y_0$ dados, el vector gradiente $\\left[ \\dfrac{\\partial z}{\\partial x}, \\dfrac{\\partial z}{\\partial y}\\right]^{\\top}_{(x_0, y_0)}$ apunta en la dirección de $x$ e $y$ para la que $z$ aumenta más. Es importante tener en cuenta que los vectores de gradiente sólo ven sus valores locales, ¡no todo el paisaje! Además, la longitud (tamaño) de cada vector, que indica la inclinación de la función, puede ser muy pequeña cerca de las mesetas locales (es decir, los mínimos o los máximos).\n",
        "\n",
        "Por lo tanto, podemos utilizar simplemente la fórmula mencionada para encontrar los mínimos locales. \n",
        "\n",
        "En 1847, Augustin-Louis Cauchy utilizó la **negativa de los gradientes** para desarrollar el algoritmo de Descenso de Gradientes como método **iterativo** para **minimizar** una **función continua** e (idealmente) **diferenciable** de **muchas variables**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "x_eJWg3jeKiT"
      },
      "source": [
        "## Sección 1.2: Algoritmo de ascenso gradual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dC5ruNxneKiT"
      },
      "source": [
        "Sea $f(\\mathbf{w}): \\mathbb{R}^d \\rightarrow \\mathbb{R}$ una función diferenciable. El descenso de gradiente es un algoritmo iterativo para minimizar la función $f$, partiendo de un valor inicial para las variables $\\mathbf{w}$, dando pasos de tamaño $\\eta$ (tasa de aprendizaje) en la dirección del gradiente negativo en el punto actual para actualizar las variables $\\mathbf{w}$.\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f \\left( \\mathbf{w}^{(t)} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "donde $\\eta > 0$ y $\\nabla f (\\mathbf{w})= \\left( \\frac{\\partial f(\\mathbf{w})}{\\partial w_1}, ..., \\frac{\\partial f(\\mathbf{w})}{\\partial w_d} \\right)$. Como Los gradientes negativos siempre apuntan localmente en la dirección del descenso más pronunciado, el algoritmo da pequeños pasos en cada punto **hacia** el mínimo.\n",
        "  \n",
        "<br/>\n",
        "\n",
        "**Vanilla Algorithm**\n",
        "\n",
        "---\n",
        "> **Inputs:** initial guess $\\mathbf{w}^{(0)}$, step size $\\eta > 0$, number of steps $T$.\n",
        "\n",
        "> **For** $t = 0, 1, 2, \\dots , T-1$ **do** \\\n",
        "$\\qquad$ $\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f \\left( \\mathbf{w}^{(t)} \\right)$\\\n",
        "**end**\n",
        "\n",
        "> **Return:** $\\mathbf{w}^{(t+1)}$\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "Por lo tanto, todo lo que necesitamos es calcular el gradiente de la función de pérdida con respecto a los parámetros aprendibles (es decir, los pesos):\n",
        "\n",
        "\\begin{equation}\n",
        "\\dfrac{\\partial Loss}{\\partial \\mathbf{w}} = \\left[ \\dfrac{\\partial Loss}{\\partial w_1}, \\dfrac{\\partial Loss}{\\partial w_2} , \\dots, \\dfrac{\\partial Loss}{\\partial w_d} \\right]^{\\top}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0dfR_6C2eKiW"
      },
      "source": [
        "### Ejercicio analítico 1.2: Gradientes\n",
        "\n",
        "Dado $f(x, y, z) = \\tanh \\left( \\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)$, cuán facil resulta derivar $\\dfrac{\\partial f}{\\partial x}$, $\\dfrac{\\partial f}{\\partial y}$ and $\\dfrac{\\partial f}{\\partial z}$?\n",
        "\n",
        "**Pista:** No hace falta que los calcule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "A-rBdYukeKiX"
      },
      "source": [
        "## Sección 1.3: Grafos Computacionales y Retropropagación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "y-OxGqbDeKiY"
      },
      "source": [
        "El *ejercicio 1.2* es un ejemplo de lo abrumadora que puede llegar a ser la derivación de gradientes, a medida que aumenta el número de variables y funciones anidadas. Esta función sigue siendo extraordinariamente sencilla en comparación con las funciones de pérdida de las redes neuronales modernas. Entonces, ¿cómo podemos (al igual que PyTorch y otros marcos similares) acercarnos a estas bestias?\n",
        "\n",
        "Veamos la función de nuevo:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x, y, z) = \\tanh \\left(\\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)\n",
        "\\end{equation}\n",
        "\n",
        "Podemos construir el llamado gráfo computacional (que se muestra a continuación) para dividir la función original en expresiones más pequeñas y accesibles.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/comput_graph.png\" alt=\"Computation Graph\" width=\"700\"/></center>\n",
        "\n",
        "Partiendo de $x$, $y$ y $z$ y siguiendo las flechas y expresiones, se vería que nuestra gráfica devuelve la misma función que $f$. Lo hace calculando las variables intermedias $a,b,c,d,$ y $e$. A esto se le llama el **paso hacia delante**.\n",
        "\n",
        "Ahora, empecemos desde $f$, y trabajemos contra las flechas mientras calculamos el gradiente de cada expresión a medida que avanzamos. Esto se llama el **paso hacia atrás**, de donde el algoritmo de **propagación hacia atrás de los errores** recibe su nombre.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/comput_graph_full.png\" alt=\"Computation Graph full\" width=\"1200\"/></center>\n",
        "\n",
        "Al dividir el cálculo en operaciones simples sobre variables intermedias, podemos utilizar la regla de la cadena para calcular cualquier gradiente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial e}~\\dfrac{\\partial e}{\\partial d}~\\dfrac{\\partial d}{\\partial c}~\\dfrac{\\partial c}{\\partial a}~\\dfrac{\\partial a}{\\partial x} = \\left( 1-\\tanh^2(e) \\right) \\cdot \\frac{1}{d+1}\\cdot z \\cdot \\frac{1}{b} \\cdot 2\n",
        "\\end{equation}\n",
        "\n",
        "Convenientemente, los valores de $e$, $b$ y $d$ están disponibles para nosotros desde que hicimos el pase hacia adelante a través del gráfico. Es decir, las derivadas parciales tienen expresiones simples en términos de las variables intermedias $a,b,c,d,e$ que calculamos y almacenamos durante el paso hacia adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-8kxpw3reKiZ"
      },
      "source": [
        "### Ejercicio analítico 1.3: Regla de la cadena (opcional)\n",
        "Para la función anterior, calcule la $\\dfrac{\\partial f}{\\partial y}$ utilizando el gráfico computacional y la regla de la cadena.\n",
        "\n",
        "---\n",
        "#### Solución:\n",
        "\\begin{equation}\n",
        "\\dfrac{\\partial f}{\\partial y} = \\dfrac{\\partial f}{\\partial e}~\\dfrac{\\partial e}{\\partial d}~\\dfrac{\\partial d}{\\partial c}~\\dfrac{\\partial c}{\\partial b}~\\dfrac{\\partial b}{\\partial y} = \\left( 1-\\tanh^2(e) \\right) \\cdot \\frac{1}{d+1}\\cdot z \\cdot \\frac{-a}{b^2} \\cdot \\cos(y)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-OGQQ1C0eKiZ"
      },
      "source": [
        "For more: [Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UdwXX7kBeKiZ"
      },
      "source": [
        "---\n",
        "# Sección 2: PyTorch AutoGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lY9eaUBHeKia"
      },
      "source": [
        "Los frameworks de aprendizaje profundo como PyTorch, JAX y TensorFlow vienen con un conjunto de algoritmos muy eficientes y sofisticados, comúnmente conocidos como diferenciación automática. AutoGrad es el motor de diferenciación automática de PyTorch. Aquí comenzamos cubriendo lo esencial de AutoGrad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JUUItL-OeKia"
      },
      "source": [
        "## Sección 2.1: Propagación hacia delante\n",
        "\n",
        "Todo comienza con la propagación hacia adelante (pass). PyTorch rastrea todas las instrucciones, a medida que declaramos las variables y operaciones, y construye el gráfico cuando llamamos al pase `.backward()`. PyTorch reconstruye el gráfico cada vez que iteramos o cambiamos (o dicho de forma sencilla, PyTorch utiliza un gráfico dinámico).\n",
        "\n",
        "Para el descenso de gradiente, sólo se requiere tener los gradientes de la función de coste con respecto a las variables que deseamos aprender. Estas variables suelen llamarse \"parámetros aprendibles / entrenables\" o simplemente \"parámetros\" en PyTorch. En las redes neuronales, los pesos y los sesgos suelen ser los parámetros aprendibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6dJHvuHHeKib"
      },
      "source": [
        "### Ejercicio 2.1: Construir un gráfico computacional\n",
        "\n",
        "En PyTorch, para indicar que un determinado tensor contiene parámetros aprendibles, podemos establecer el argumento opcional `requires_grad` a `True`. PyTorch rastreará entonces cada operación que utilice este tensor mientras configura el gráfico computacional. Para este ejercicio, utiliza los tensores proporcionados para construir el siguiente gráfico, que implementa una sola neurona con entrada y salida escalar.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/simple_graph.png\" alt=\"Simple nn graph\" width=\"600\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "2TiyUwh6eKib"
      },
      "outputs": [],
      "source": [
        "class SimpleGraph:\n",
        "  \"\"\"\n",
        "  Implementing Simple Computational Graph\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, w, b):\n",
        "    \"\"\"\n",
        "    Initializing the SimpleGraph\n",
        "\n",
        "    Args:\n",
        "      w: float\n",
        "        Initial value for weight\n",
        "      b: float\n",
        "        Initial value for bias\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    assert isinstance(w, float)\n",
        "    assert isinstance(b, float)\n",
        "    self.w = torch.tensor([w], requires_grad=True)\n",
        "    self.b = torch.tensor([b], requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass\n",
        "\n",
        "    Args:\n",
        "      x: torch.Tensor\n",
        "        1D tensor of features\n",
        "\n",
        "    Returns:\n",
        "      prediction: torch.Tensor\n",
        "        Model predictions\n",
        "    \"\"\"\n",
        "    assert isinstance(x, torch.Tensor)\n",
        "    #################################################\n",
        "    ## Implement the the forward pass to calculate prediction\n",
        "    ## Note that prediction is not the loss, but the value after `tanh`\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
        "    #################################################\n",
        "    prediction = ...\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def sq_loss(y_true, y_prediction):\n",
        "  \"\"\"\n",
        "  L2 loss function\n",
        "\n",
        "  Args:\n",
        "    y_true: torch.Tensor\n",
        "      1D tensor of target labels\n",
        "    y_prediction: torch.Tensor\n",
        "      1D tensor of predictions\n",
        "\n",
        "  Returns:\n",
        "    loss: torch.Tensor\n",
        "      L2-loss (squared error)\n",
        "  \"\"\"\n",
        "  assert isinstance(y_true, torch.Tensor)\n",
        "  assert isinstance(y_prediction, torch.Tensor)\n",
        "  #################################################\n",
        "  ## Implement the L2-loss (squred error) given true label and prediction\n",
        "  # Complete the function and remove or comment the line below\n",
        "  raise NotImplementedError(\"Loss function `sq_loss`\")\n",
        "  #################################################\n",
        "  loss = ...\n",
        "  return loss\n",
        "\n",
        "feature = torch.tensor([1])  # Input tensor\n",
        "target = torch.tensor([7])  # Target tensor\n",
        "## Uncomment to run\n",
        "# simple_graph = SimpleGraph(-0.5, 0.5)\n",
        "# print(f\"initial weight = {simple_graph.w.item()}, \"\n",
        "#       f\"\\ninitial bias = {simple_graph.b.item()}\")\n",
        "# prediction = simple_graph.forward(feature)\n",
        "# square_loss = sq_loss(target, prediction)\n",
        "# print(f\"for x={feature.item()} and y={target.item()}, \"\n",
        "#       f\"prediction={prediction.item()}, and L2 Loss = {square_loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yrod_CK8eKic"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial1_Solution_66660afb.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "RMGmppnDeKic"
      },
      "source": [
        "Es importante apreciar el hecho de que PyTorch puede seguir nuestras operaciones mientras recorremos arbitrariamente las clases y funciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OsrwmdzQeKic"
      },
      "source": [
        "## Sección 2.2: Propagación hacia atrás\n",
        "\n",
        "Aquí es donde reside toda la magia. En PyTorch, `Tensor` y `Función` están interconectados y construyen un gráfico acíclico, que codifica una historia completa de computación. Cada variable tiene un atributo `grad_fn` que hace referencia a una función que ha creado el Tensor (excepto los Tensores creados por el usuario - estos tienen `None` como `grad_fn`).  El ejemplo siguiente muestra que el tensor `c = a + b` es creado por la operación `Add` y la función de gradación es el objeto `<AddBackward...>`. Sustituye `+` por otras operaciones simples (por ejemplo, `c = a * b` o `c = torch.sin(a)`) y examina los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "JKOVIJtkeKic"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0], requires_grad=True)\n",
        "b = torch.tensor([-1.0], requires_grad=True)\n",
        "c = a + b\n",
        "print(f'Gradient function = {c.grad_fn}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NiliDJQdeKid"
      },
      "source": [
        "Para funciones más complejas, la impresión del `grad_fn` sólo mostraría la última operación, aunque el objeto rastrea todas las operaciones hasta ese punto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "makeYNBxeKid"
      },
      "outputs": [],
      "source": [
        "print(f'Gradient function for prediction = {prediction.grad_fn}')\n",
        "print(f'Gradient function for loss = {square_loss.grad_fn}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ShrZIJOGeKid"
      },
      "source": [
        "Ahora vamos a iniciar el paso hacia atrás para calcular los gradientes llamando a `.backward()` en el tensor desde el que queremos iniciar la retropropagación. A menudo, `.backward()` se llama a la pérdida, que es el último nodo en el gráfico. Antes de hacer eso, vamos a calcular los gradientes de pérdida a mano:\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = - 2 x (y_t - y_p)(1 - y_p^2)$$\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = - 2 (y_t - y_p)(1 - y_p^2)$$\n",
        "\n",
        "Donde $y_t$ es el objetivo (etiqueta verdadera), y $y_p$ es la predicción (salida del modelo). Luego podemos compararlo con los gradientes de PyTorch, que pueden obtenerse llamando a `.grad` en los tensores relevantes.\n",
        "\n",
        "**Notas importantes:**\n",
        "* Los parámetros aprendibles (es decir, los tensores `requires_grad`) son \"contagiosos\". Veamos un ejemplo sencillo: `Y = W @ X`, donde `X` es el tensor de características y `W` es el tensor de pesos (parámetros aprendibles, `requires_grad`), el nuevo tensor de salida generado `Y` será también `requires_grad`. Así que cualquier operación que se aplique a `Y` formará parte del grafo computacional. Por lo tanto, si necesitamos trazar o almacenar un tensor que es `requires_grad`, primero debemos `.detach()` del gráfico llamando al método `.detach()` sobre ese tensor.\n",
        "\n",
        "* `.backward()` acumula gradientes en los nodos hoja (es decir, los nodos de entrada al nodo de interés). Podemos llamar a `.zero_grad()` en la pérdida u optimizador para poner a cero todos los atributos `.grad` (ver [autograd.backward](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) para más información).\n",
        "\n",
        "* Recordemos que en python podemos acceder a las variables y a los métodos asociados con `.method_name`. Puedes utilizar el comando `dir(mi_objeto)` para observar todas las variables y métodos asociados a tu objeto, por ejemplo, `dir(simple_graph.w)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "LxooD6TjeKie"
      },
      "outputs": [],
      "source": [
        "# Analytical gradients (Remember detaching)\n",
        "ana_dloss_dw = - 2 * feature * (target - prediction.detach())*(1 - prediction.detach()**2)\n",
        "ana_dloss_db = - 2 * (target - prediction.detach())*(1 - prediction.detach()**2)\n",
        "\n",
        "square_loss.backward()  # First we should call the backward to build the graph\n",
        "autograd_dloss_dw = simple_graph.w.grad  # We calculate the derivative w.r.t weights\n",
        "autograd_dloss_db = simple_graph.b.grad  # We calculate the derivative w.r.t bias\n",
        "\n",
        "print(ana_dloss_dw == autograd_dloss_dw)\n",
        "print(ana_dloss_db == autograd_dloss_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "v5sp46WPeKie"
      },
      "source": [
        "## Referencias y más:\n",
        "\n",
        "* [A gentle introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "* [Automatic Differentiation package - torch.autograd](https://pytorch.org/docs/stable/autograd.html)\n",
        "\n",
        "* [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n",
        "\n",
        "* [Automatic Differentiation with torch.autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "7O8GxQCoeKie"
      },
      "source": [
        "---\n",
        "# Sección 3: Módulo de red neuronal de PyTorch (`nn.Module`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "J90RgwyJeKif"
      },
      "source": [
        "PyTorch nos proporciona bloques de construcción de redes neuronales listos para usar, como capas (por ejemplo, lineales, recurrentes, etc.), diferentes funciones de activación y pérdida, y mucho más, empaquetados en el módulo [`torch.nn`](https://pytorch.org/docs/stable/nn.html). Si construimos una red neuronal utilizando las capas de `torch.nn`, los pesos y los sesgos ya están en modo `requires_grad` y se registrarán como parámetros del modelo. \n",
        "\n",
        "Para el entrenamiento, necesitamos tres cosas:\n",
        "\n",
        "**Parámetros del modelo:** Los parámetros del modelo se refieren a todos los parámetros aprendibles del modelo, que son accesibles llamando a `.parameters()` en el modelo. Ten en cuenta que NO todos los tensores `requires_grad` se consideran parámetros del modelo. Para crear un parámetro de modelo personalizado, podemos utilizar [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) (*Un tipo de Tensor que debe ser considerado un parámetro del módulo*).\n",
        "\n",
        "* **Función de pérdida:** La pérdida que vamos a optimizar, que a menudo se combina con los términos de regularización (en unos días).\n",
        "\n",
        "* **Optimizador:** PyTorch nos proporciona muchos métodos de optimización (diferentes versiones del descenso de gradiente). El optimizador mantiene el estado actual del modelo y llamando al método `step()`, actualizará los parámetros basándose en los gradientes calculados.\n",
        "\n",
        "Aprenderás más detalles sobre la elección de la arquitectura del modelo, la función de pérdida y el optimizador adecuados más adelante en el curso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xXjKjhfVeKif"
      },
      "source": [
        "## Sección 3.1: Bucle de entrenamiento en PyTorch\n",
        "\n",
        "Utilizamos un problema de regresión para estudiar el bucle de entrenamiento en PyTorch.\n",
        "\n",
        "La tarea es entrenar una red neuronal amplia no lineal (utilizando la función de activación $\\tanh$) para una tarea de regresión simple $\\sin$. Se cree que las redes neuronales anchas son muy buenas para la generalización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FIWFB3z6eKig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "cf0ec193-0b3b-4c5d-ebba-2f4917b37ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABFwAAAKrCAYAAADMPplSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdf5TkaV0f+vcztLDS7hQFamxAt+SqDTbaumjgCs6ALUMgHEJCiCRc1Oy6m6DGGMOJ56g7rLP+SG70xvgruuuu4pWIGGPQCDihhRn8gT9WbGGiDeqWAjbGC0UPFAS2mef+UdVuM9s93V39repfr9c5derH9/s8389Az56u9zzP51tqrQEAAACgOSf2uwAAAACAo0bgAgAAANAwgQsAAABAwwQuAAAAAA0TuAAAAAA0TOACAAAA0DCBCwAAAEDDBC4AAAAADRO4AAAAADRM4AIAAADQMIELAAAAQMMELsdcKeWVpZRX7ncdAAAAcJRM7XcB7LvH33jjjTcm+Sf7XQgAAAAcQGWUQVa4AAAAADRM4AIAAADQMIELAAAAQMMELgAAAAANE7gAAAAANEzgAgAAANAwgQsAAABAwwQuAAAAAA0TuAAAAAA0TOACAAAA0DCBCwAAAEDDBC4AAAAADRO4AAAAADRM4AIAAADQMIELAAAAQMMELgAAAAANE7gAAAAANEzgAgAAANAwgQsAAABAw6b2uwAAAICj4lK/n8VeL5fX1nJyaioL7Xbmpqf3uyxgHwhcAAAA9mix18u5bjcXV1cfdOxUq5WznU4W2u19qAzYL7YUAQAA7MHdKys5s7S0adiSJBdXV3NmaSn3rKxMuDJgPwlcAAAARrTY6+XW5eVc2ea8K0luWV7OYq83ibKAA0DgAgAAMKJz3e62Ycu6K0nu6HbHWA1wkAhcAAAARnCp399yG9FWLqyu5lK/P6aKgINE4AIAADCCUbcH2VYEx4PABQAAYASX19YmOg44XAQuAAAAIzg5NTXRccDhInABAAAYwUK7PdFxwOEicAEAABjB3PR0TrVauxpzutXK3PT0mCoCDhKBCwAAwIjOdjo7/lJ1Isltnc4YqwEOEoELAADAiBba7dw5O7vtF6sTSe6anbWdCI4RgQsAAMAe3Dwzk/Pz8zm9xfai061Wzs/P56aZmQlXBuwn7bEBAAD2aKHdzkK7nUv9fhZ7vVxeW8vJqakstNt6tsAxJXABAABoyNz0tIAFSGJLEQAAAEDjBC4AAAAADRO4AAAAADRM4AIAAADQMIELAAAAQMMELgAAAAANE7gAAAAANEzgAgAAANAwgQsAAABAwwQuAAAAAA0TuAAAAAA0TOACAAAA0DCBCwAAAEDDBC4AAAAADRO4AAAAADRM4AIAAADQMIELAAAAQMMELgAAAAANm9rvAgAA4KC51O9nsdfL5bW1nJyaykK7nbnp6f0uC4BDROACAABDi71eznW7ubi6+qBjp1qtnO10stBu70NlABw2thQBAECSu1dWcmZpadOwJUkurq7mzNJS7llZmXBlABxGAhcAAI69xV4vty4v58o2511JcsvychZ7vUmUBcAhJnABAODYO9ftbhu2rLuS5I5ud4zVAHAUCFwAADjWLvX7W24j2sqF1dVc6vfHVBEAR4HAZRdKKQ8vpTy7lPIdpZT/Wkr581JKHT5ub+gaf6uU8v2llOVSykdKKe8vpby5lPJ1pZTSxDUAAHjAqNuDbCsC4FrcpWh3/naS145r8lLKk5L8apJHDT/6UJLrkzxt+PiHpZTn1Vo/Nq4aAACOm8traxMdB8DxIHDZvV6S39/w+A9JPmOvk5ZSWkn+ewZhyx8neUmt9fdKKQ9NcsvwOs9K8gNJvn6v1wMAYODk1Gi/Eo86biuX+v0s9nq5vLaWk1NTWWi3Mzc93eg1OHr83MDBJXDZnTfXWh+58YNSyr9taO6XZRDcfCTJc2qt9yXJcDXLj5RSTib5niS3llJ+oNb6joauCwBwrC202xMdd7XFXi/nut1N+8icarVyttNp7FocHX5u4ODTw2UXaq0fH+P0Xz18ftV62HKVH8pgi9FDkrx4jHUAABwrc9PTOdVq7WrM6VarkVUEd6+s5MzS0pZNey+urubM0lLuWVnZ87U4OvzcwOEgcDkASimzST5r+PZ1m51Ta/1QkjcP356ZRF0AAMfF2U5nx78Yn0hyW6ez52su9nq5dXl529tRX0lyy/KyJr0k8XMzKZf6/fzgu9+d7+p284Pvfre7kjESW4oOhidueP32a5z39iTPTvL54y0HAOB4WWi3c+fs7LZfZE8kuWt2tpGtGue63W2/NK+7kuSObtcWEfzcjJmtWjRJ4HIwPHrD6/dc47z1YydLKZ8yXPWypVLKvTu49uN3cA4AwJF388xMOtddlzu63VzY5MvW6VYrtzX0ZetSv7/ldpCtXFhdzaV+X0PUY8zPzXjdvbJyzdB1favWXbOzuWlmZqK1cTgJXA6G6ze8/vA1ztt47PoMeroAANCQhXY7C+322O/8Muo2j8VezxfnY8zPzfjsdqvWDdddZ6UL2xK4HGG11idtd85wFcyNEygHAODQmJueHusX1MtraxMdx9Hg52Z8bNViHDTNPRg+uOH1w69x3sZjH9zyLAAADrSTU6P9u+eo4zga/NyMx162asG1CFwOhr/c8Pox1zhv/djl7fq3AABwcI36L+P+Rf1483MzHnvZqgXXInA5GDbemeiJW571wLH/OcZaAAAYs7np6ZxqtXY15nSrpQ/HMefnZjxs1WJcBC4HwzuS/MXw9d/Z7IRSynSSLx++PT+JogAAGJ+znc6Ofxk/keS2TmeM1XBY+Llpnq1ajIvA5QCotdYkPz18+6JSSmeT074hyack+XiSV06mMgAAxmWh3c6ds7Pb/kJ+Islds7O2hZDEz8042KrFuAhcdqmU0i6lfOr6Iw/8b/jwjZ+XUj7lqnG3l1Lq8NHZZOrvS/LeDBrj/kop5UnDcQ8tpbw0yR3D8+6stb5jHH82AAAm6+aZmZyfn8/pLbaJnG61cn5+PjfNzEy4Mg4yPzfNslWLcSmDxRXsVCmlm+SGHZz6ilrr124Yd3uSlw/ffnattbvJ3E9K8qtJHjX86INJrkvyScP355M8r9b60RFK31Qp5d4bb7zxxnvvvbepKQEAGMGlfj+LvV4ur63l5NRUFtptX+jYlp+bZiz2ejmztLSjW0OfSHJ+ft4Kl+OljDLIprMDpNZ6byllLsm3Jnluks9M0s+gqe4rktxTa93p7eEBADhE5qanfVFm1/zcNGN9q9aty8vXDF1s1WI3rHA55qxwAQAAGFjs9XJHt5sLq6sPOna61cptnY6w5XiywgUAAABGtdBuZ6HdtlWLRghcAAAAYANbtWiCwAUAADiSrFIA9pPABQAAOFIWe72c63ZzcZM+HKdarZzVhwOYgBP7XQAAAEBT7l5ZyZmlpU3DliS5uLqaM0tLuWdlZcKVAceNwAUAADgSFnu9bW/rmyRXktyyvJzFXm8SZQHHlC1FAADAkXCu2902bFl3Jckd3a6tRYeAXjwcVgIXAADg0LvU72+5jWgrF1ZXc6nf9+X9gNKLh8POliIAAODQG3V7kG1FB5NePBwFAhcAAODQu7y2NtFxjI9ePBwVAhcAAODQOzk1WreEUccxPqP04oGDSOACAAAceqP28tAD5GDZSy8eOGgELgAAwKE3Nz2dU63WrsacbrU0zD1g9OLhKBG4AAAAR8LZTmfHX3BOJLmt0xljNYxCLx6OEoELAABwJCy027lzdnbbLzknktw1O2s70QGkFw9HicAFAAA4Mm6emcn5+fmc3mJ70elWK+fn53PTzMyEK2Mn9OLhKBEDAgAAR8pCu52FdjuX+v0s9nq5vLaWk1NTWWi39Ww54NZ78eymca5ePBxUAhcAAOBImpue9kX8EDrb6eTM0tKObg2tFw8HmS1FAAAAHBh68XBUCFwAAAA4UPTi4SiwpQgAAIADRy8eDjuBCwAAAAeWXjwcVrYUAQAAADTMChcAADiGbNMAGC+BCwAAHCOLvV7Odbu5uLr6oGOnWq2c7XTc9QWgAbYUAQDAMXH3ykrOLC1tGrYkycXV1ZxZWso9KysTrgzg6BG4AADAMbDY6+XW5eVc2ea8K0luWV7OYq83ibIAjiyBCwAAHAPnut1tw5Z1V5Lc0e2OsRqAo0/gAgAAR9ylfn/LbURbubC6mkv9/pgqAjj6BC4AAHDEjbo9yLYigNEJXAAA4Ii7vLY20XEACFwAAODIOzk1NdFxAAhcAADgyFtotyc6DgCBCwAAHHlz09M51WrtaszpVitz09Njqgjg6BO4AADAMXC209nxL/8nktzW6YyxGoCjT+ACAADHwEK7nTtnZ7f9AnAiyV2zs7YTAeyRwAUAAI6Jm2dmcn5+Pqe32F50utXK+fn53DQzM+HKAI4ebccBAOAYWWi3s9Bu51K/n8VeL5fX1nJyaioL7baeLQANErgAAMAxNDc9LWABGCOBCwAAB57VGAAcNgIXAAAOrMVeL+e63VxcXX3QsVOtVs52Opq7AnAgaZoLAMCBdPfKSs4sLW0atiTJxdXVnFlayj0rKxOuDAC2J3ABAODAWez1cuvycq5sc96VJLcsL2ex15tEWQCwYwIXAAAOnHPd7rZhy7orSe7odsdYDQDsnsAFAIAD5VK/v+U2oq1cWF3NpX5/TBUBwO4JXAAAOFBG3R5kWxEAB4nABQCAA+Xy2tpExwHAOAhcAAA4UE5OTU10HACMg8AFAIADZaHdnug4ABgHgQsAAAfK3PR0TrVauxpzutXK3PT0mCoCgN0TuAAAcOCc7XR2/IvqiSS3dTpjrAYAdk/gAgDAgbPQbufO2dltf1k9keSu2VnbiQA4cHQWAwDgQLp5Ziad667LHd1uLqyuPuj46VYrt3U6wpZD5FK/n8VeL5fX1nJyaioL7batYMCRJXABAODAWmi3s9Bu+6J+yC32ejnX7ebiJsHZqVYrZwVnwBEkcAEA4MCbm54WsBxSd6+s5Nbl5VzZ4vjF1dWcWVrKXbOzuWlmZqK1AYyTHi4AAMBYLPZ61wxb1l1JcsvychZ7vUmUBTARAhcAAGAsznW724Yt664kuaPbHWM1AJMlcAEAABp3qd/ftGfLtVxYXc2lfn9MFQFMlsAFAABo3Kjbg2wrAo4KgQsAANC4y2trEx0HcNAIXAAAgMadnBrthqijjgM4aAQuAABA4xba7YmOAzhoBC4AAEDj5qanc6rV2tWY061W5qanx1QRwGQJXAAAgLE42+ns+AvHiSS3dTpjrAZgsgQuAADAWCy027lzdnbbLx0nktw1O2s7EXCkCFwAAICxuXlmJufn53N6i+1Fp1utnJ+fz00zMxOuDGC8tAAHAADGaqHdzkK7nUv9fhZ7vVxeW8vJqakstNt6tkDi78YRJXABAAAmYm562pdI2GCx18u5bjcXV1cfdOxUq5WznY6tdoeYLUUAAAAwYXevrOTM0tKmYUuSXFxdzZmlpdyzsjLhymiKwAUAAAAmaLHXy63Ly7myzXlXktyyvJzFXm8SZdEwgQsAAABM0Llud9uwZd2VJHd0u2OshnERuAAAAMCEXOr3t9xGtJULq6u51O+PqSLGReACAAAAEzLq9iDbig4fgQsAAABMyOW1tYmOY/8IXAAAAGBCTk5NTXQc+0fgAgAAABOy0G5PdBz7R+ACAAAAEzI3PZ1TrdauxpxutTI3PT2mihgXgQsAAABM0NlOZ8dfxk8kua3TGWM1jIvABQAAACZood3OnbOz234hP5HkrtlZ24kOKYELAAAATNjNMzM5Pz+f01tsLzrdauX8/HxumpmZcGU0RZtjAAAA2AcL7XYW2u1c6vez2Ovl8tpaTk5NZaHd1rPlCBC4AAAAwD6am54WsBxBApcRlFKuT/Kvk7wgyWcn+XiSdyR5VZIfqrV+bIQ5b0/y8h2c+rm11j/Z7fwAAE3xL7EAsD2Byy6VUm5I8qYkneFHH07ysCRfMny8uJSyUGvtjXiJ+5O8/xrH10acFwBgTxZ7vZzrdnNxdfVBx061Wjnb6WjsCABDmubuQillKskvZxC2rCR5Zq11OsnDk7woyQeTfHGSn9nDZX6z1voZ13h09/anAADYvbtXVnJmaWnTsCVJLq6u5szSUu5ZWZlwZQBwMAlcdudrknzB8PULaq1vSJJa65Va688l+WfDY88ppSzsR4EAAE1b7PVy6/Jyrmxz3pUktywvZ7E36kJfADg6BC678zXD5zfWWn9rk+OvSnLf8PVXT6YkAIDxOtftbhu2rLuS5I5ud4zVAMDhoIfLDpVSHp7kqcO3r9vsnFprLaW8PslLk5yZVG0AAONyqd/fchvRVi6sruZSv6+RLhxRGmfDzghcdu4JeWBF0Nuvcd76sc8opTyy1nqtBribmSulvD3J4zL4R6L3JLmY5EdrrW/dzUSllHt3cNrjd1kfAHCMjLo9aLHX8wUMjhiNs2F3bCnauUdveP2ea5y38dijtzxra5+aQbjzkQzufvR5Sb4uyb2llO8aYT4AgJFdXhvtBomjjgMOJo2zYfescNm56ze8/vA1ztt47Potz3qwdyb5N0lek+S+Wuv9pZSHJnl6ku9J8qQk315K6dVav38nE9Zan7TdOcNVMDfuok4A4Bg5OTXar4ujjgMOnt02zr7huuusdIFY4XJg1FpfWWv997XWd9Ra7x9+9rFa6/kkT0vyu8NTby+ltPatUADgWBn1S5MvW3B0aJwNoxG47NwHN7x++DXO23jsg1uetQu11v+d5NuGbz8liVtOAwATMTc9nVOt3f1bz+lWS/8WOCL20jgbjjuBy8795YbXj7nGeRuP/eWWZ+3exttQP67BeQEArulsp7PjXxpPJLmt0xljNcAk7aVxNhx3Aped+6Pkb1bSPfEa560fe+8IdygCADhwFtrt3Dk7u+0vjieS3DU7azsRHCEaZ8PoBC47VGv9cJLfGL79O5udU0opSZ41fHu+4RKesuH1fQ3PDQBwTTfPzOT8/HxOb7G96HSrlfPz87lpZmbClQHjpHE2jM7fgt15RZIvT/KMUsqTa62/fdXxF+aB7T4/vdNJSyml1lqvcfxhSb57+LafZHHnJQMANGOh3c5Cu51L/X4We71cXlvLyampLLTberbAEaVxNozOCpfdeUWStyUpSX6hlLKQJKWUE6WUFya5a3je62qtnxCKlFJuL6XU4aNz1bynSilvKKW8pJTy2A1jPml4jTcnefLw43O11g80/icDANihuenpfNNjH5vv6HTyTY99rLAFjjCNs2F0VrjsQq11rZTyvCRvTNJJ8oZSyoczCK6uG5721iQv3uXUJYM7D60HOB/JYCVLK8knDc+5kuTf1lr/7738GQAAAHbjbKeTM0tLO7o1tMbZ8AArXHap1tpN8oVJziV5e5Ka5P4k9yZ5WZKn1Fp325L7bcOxv5DkHUk+kuQRw+elJD+c5Itqrd/ewB8BAABgxzTOhtGUa7QO4Rgopdx744033njvvffudykAAMABttjr5Y5uNxdWVx907HSrlds6HWELR1UZZZAtRQAAAGxL42zYHYELAAAAOzY3PS1ggR3QwwUAAACgYVa4AAA0yFJ7ACARuAAANGKx18u5bjcXN2kmearVylnNJAHgWLGlCABgj+5eWcmZpaVNw5Ykubi6mjNLS7lnZWXClQEA+0XgAgCwB4u9Xm5dXs6Vbc67kuSW5eUs9nqTKAsA2GcCFwCAPTjX7W4btqy7kuSObneM1QAAB4XABQBgRJf6/S23EW3lwupqLvX7Y6oIADgoBC4AACMadXuQbUUAcPQJXAAARnR5bW2i4wCAw0PgAgAwopNTUxMdBwAcHgIXAIARLbTbEx0HABweAhcAgBHNTU/nVKu1qzGnW63MTU+PqSIA4KAQuAAA7MHZTmfHv1CdSHJbpzPGagCAg0LgAgCwBwvtdu6cnd32l6oTSe6anbWdCACOCYELAMAe3Twzk/Pz8zm9xfai061Wzs/P56aZmQlXBgDsFy3yAQAasNBuZ6HdzqV+P4u9Xi6vreXk1FQW2m09WwDgGBK4AAA0aG56WsACANhSBAAAANA0gQsAAABAwwQuAAAAAA0TuAAAAAA0TOACAAAA0DCBCwAAAEDDBC4AAAAADRO4AAAAADRM4AIAAADQMIELAAAAQMMELgAAAAANE7gAAAAANEzgAgAAANAwgQsAAABAwwQuAAAAAA2bGvcFSikzSdpJPjnJR5K8v9b63nFfFwAAAGC/NBq4lFIenuQrkzwryZOTzCV56CbnfSzJ25P8dpJfTfKGWutHmqwFAAAAYL80EriUUp6S5OuT/P0kD994aIshD0ty4/Dx0iT9Usp/TfJjtda3NFETAAAAwH7ZU+BSSnlmku/MYDVL8okBy0eT/FmSXpL3JbmcpJXkkcPH4/LA6pdPSfKSJC8ppbwlyctrrW/YS20AAAAA+2WkwKWU8oQkP5jkK/JAyPLRJK9L8sYkb0nyB7XW+68xx0OTfFEGYc0zkjw7g5Uv/2eSXy2lLCb5plrrH49SIwAAAMB+GXWFy1KSh2QQtvx6kp9O8l9qrR/Y6QS11o8l+Z3h44dKKa0kL8xgpcuXZ9AL5g+SXDdijQAAAAD7YtTbQk9lsJLlVK31VK31J3YTtmym1ro6nOd0BoHLr2WThrsAAAAAB92oK1y+vNb6G41WssFw7q8spTx1XNcAAAAAGJeRVriMM2zZj+sAAAAANGnULUUAAAAAbEHgAgAAANCwUXu4jKyU8rAkNyd52vD6S0nurLX+9aRrAQAAABiHRgOXUsp8kp9MUpN8Q631LVcdvz7JhSTzGz5+QZJvKqU8s9b6h03WAwAAALAfmt5S9I+SfFGSRyf57U2Of8/weLnq8WlJfnG4+gUAAADgUGs6cHlKBqtbztda68YDpZSTSb5uePy+JM9N8sQkPzo8pZPkJQ3XAwAAADBxTQcujxk+v3WTY89Jsr6C5aZa62trrf+z1vqNSf5g+Pnfa7geAAAAgIlrOnD51OHzyibHnj58fnet9cJVx34+g61FX9hwPQAAAAAT13TgcnL4fP8mx74sg+1Ei5sce9fw+dMargcAAABg4poOXPrD50/f+GEp5VFJ5oZvf2OTcR8dUz0AAAAAE9d0wPGnw+dTV33+/Ay2DCWbBy7rAc1qw/UAAAAATFzTgcubMghWXlBKeUGSlFIem+Tbhse7tdY/3mTceu+WP2u4HgAAAICJazpw+bEkH0syleTVpZT3Z3AL6E4G/Vt+ZItxXzk8/vsN1wMAAAAwcY0GLrXWP0nyjRmEJyXJI5I8ZPj6TUl+8OoxpZQnJ/ns4ds3N1kPAAAAwH6YanrCWutPlFLuTXJTks9J8uEk/yPJ3bXWtU2GvDDJn2cQ0vxq0/UAAAAATFrjgUuS1FrfmuRf7PDclyV52TjqAAAAANgPbsMMAAAA0LBGA5dSyq+VUhZLKV+2y3Ffuj62yXoAAAAA9kPTW4qenkEvlk/d5bhHbhgLAAAAcKjZUgQAAADQsIMSuDxs+Pyxfa0CAAAAoAEHJXD5wuHz+/e1CgAAAIAGjNzDpZTyWUk6Wxx+YinlA9tNkWQ6yY1J/k0G/Vv+YNR6AAAAAA6KvTTN/adJzm7yeUlyxy7nKhkELq/YQz0AAAAAB8Je71JUdvn5Vu5P8n211lfvsR4AAACAfbeXwOVNm3z28gxWqrw6yR9vM/5Kkg8luS/Jm2ut79tDLQAAAAAHxsiBS631QpILGz8rpbx8+PJna62/tJfCAAAAAA6rvW4putp3Dp+3W90CAAAAcGQ1GrjUWr9z+7MAAAAAjramV7hsqpTysCTtJA+ttf7FJK4JAAAAsF/GFriUUp6Q5F8mOZPkhuHH9eprllK+Ksn/keS9tdZ7xlUPAAAAwKSMJXAppZxNcluSE9n+FtHXJfmuJGullF+ptf7VOGqCUVzq97PY6+Xy2lpOTk1lod3O3PT0fpcFAADAAdd44FJK+c4k35FB0PLxJL8zfH7aFkN+LskPJ3l4kr+X5M6ma4LdWuz1cq7bzcXV1QcdO9Vq5Wynk4V2ex8qAwAA4DA40eRkpZQnJvn24ds/SPL5tdanJvn+rcbUWv93ksXh26c3WQ+M4u6VlZxZWto0bEmSi6urObO0lHtWViZcGQAAAIdFo4FLkq8fztlL8qxa6zt3OO73MlgR8wUN1wO7stjr5dbl5VzZ5rwrSW5ZXs5irzeJsgAAADhkmg5cnpFBY9yfqrX+9S7Grd+56LEN1wO7cq7b3TZsWXclyR3d7hirAQAA4LBqOnB5zPD53l2O+9DwWTdS9s2lfn/LbURbubC6mkv9/pgqAgAA4LBqOnB5yPD547sc1xo+f7DBWmBXRt0eZFsRAAAAV2s6cFm/pfMNuxw3P3x+T4O1wK5cXlub6DgAAACOrqYDl9/JoPntc3c6oJTySUlemEHvl19vuJ6xKKVcX0q5vZTytlLKh0opq6WU3y2l/OtSykP3OPffKqV8fylluZTykVLK+0spby6lfF0ppTT1Z+DBTk6Ndpf0UccBAABwdDUduPzC8PlppZR/sMMx/y7JzPD1zzZcT+NKKTck+cMkL0/yxAwCpocl+ZIk35fkLaWU9ohzPynJpSTfkuTzkqwluT7J05LcleR1ew102NpCe6T/20YeBwAAwNHVdODyX5IsZRBC/Ewp5eu3CghKKY8rpfxMkn+ZweqWxVrrmxuup1GllKkkv5ykk2QlyTNrrdNJHp7kRRn0oPniJD8zwtytJP89yaOS/HGSL621Xp9BI+FvTHJ/kmcl+YE9/0HY1Nz0dE61WtufuMHpVitz03o9AwAA8IkaDVxqrTXJP0zyviTXJfmhDPq6fO/6OaWUN5ZS3pnknUn+cQbhzHuSvKTJWsbka5J8wfD1C2qtb0iSWuuVWuvPJflnw2PPKaUs7HLulyX5jCQfSfKcWuvvDef+WK31RzJYUZMkt5ZSPm8vfwi2drbT2fFfihNJbut0xlgNAAAAh1XTK1xSa/3TJE9J8tYMwpRWktkMVrEkyakkjxseK0l+N8mX1Vr/6sGzHThfM5kO2FQAACAASURBVHx+Y631tzY5/qok9w1ff/Uu514//1W11vs2Of5DGdw++yFJXrzLudmhhXY7d87ObvsX40SSu2ZnbScCAABgU40HLsnfhC5fkuQFSf5bkvfngYClJOkneW2Sr0rylFrru8dRR5NKKQ9P8tTh29dtds5whc/rh2/P7GLu2SSftc3cH0qyvuVqx3OzezfPzOT8/HxOb7G96HSrlfPz87lpZmbT4wAAADC226sMw4dfHD5SSpnOYLXLh2qtl8d13TF6Qh4IqN5+jfPWj31GKeWRtdb372DuJ24yfqu5n53k83cwZ0op9+7gtMfvZK7jZqHdzkK7nUv9fhZ7vVxeW8vJqakstNt6tgAAALCtid3Pttbaz2Bly2H16A2v33ON8zYee3QGq3uanvtkKeVThqtejo39CD/mpqcFLAAAAOzaxAKXI+D6Da8/fI3zNh67fsuzmpn7moFLrfVJ2114uArmxu3O20+LvV7Odbu5uLr6oGOnWq2c7XT0UgEAAOBAGUsPF2jK3SsrObO0tGnYkiQXV1dzZmkp96ysTLgyAAAA2FqjK1xKKWdHHHolyQcz2H7zh0neVmu90lhhzfjghtcPv8Z5G499cMuzrj33Vj1uRpn70Frs9XLr8nK2+0G4kuSW5eXccN11VroAAABwIDS9pej2PHD75734/0opdyf57mHvl4PgLze8fkwGwdBmHrPFmN3MvVXgsj735ePQv+Vct7tt2LLuSpI7ul2BCwAAAAfCOLYUbbz989Xvr35sdfzTknxrkreWUj5zDDWO4o+Sv/n+/8RrnLd+7L07vENR8ol3JtrJ3P9zh/MeWpf6/S23EW3lwupqLvUPSj4HAADAcdZ04PKM4ePnh+/XkvxSkm9J8vwkzxw+f8vw87UMVsS8enjsRUn+nyR/nUHw8jlJXlNKKdlntdYPJ/mN4du/s9k5wzqfNXx7fhfTvyPJX2wz93SSLx9h7kNpsdeb6DgAAABoUqOBS631QpK/l+QfJvn9JE+stT6/1voDtdZfqrUuDp9/oNb6/CRfkOQPkrwwyXNqra+utb4syecmecNw2vkkX9VknXvwiuHzM0opT97k+AuTPG74+qd3OmmttW44/0WllM4mp31Dkk9J8vEkr9zp3IfV5bW1iY4DAACAJjUauJRSnpnkm5P8VZKvrLW+81rn11rfkcHKlr9O8s2llK8cfn45g/BifbnCC5qscw9ekeRtGay++YVSykKSlFJOlFJemOSu4Xmvq7UubhxYSrm9lFKHj84mc39fkvdm0Bj3V0opTxqOe2gp5aVJ7hied+fwf7cj7eTUaO2FRh0HAAAATWp6S9FLM9gidHet9QM7GTDsc/ITGYQY/3zD56tJfnb4+Zc0XOdIaq1rSZ6XpJtBA9s3lFL6SfoZbIs6meStSV48wtyrSZ6b5H1JPj/J75VSLif5UJIfTfLQDLYS/as9/0EOgVGb32qaCwAAwEHQdOCyHoy8/ZpnPdj6+X/7qs9/f/j8aSNX1LBaazfJFyY5l0HdNcn9Se5N8rIkT6m1jtRIpNZ6b5K5JP8hyTuTfFIGYc6vJ7klybNrrR/d4x/hUJibns6pVmtXY063Wpmbnh5TRQAAALBzTe+/WA9GrtvluPXzrw5W1m9TM467KY2s1vrBJC8fPnY65vYMbpu93Xl/lUFT4W8Zsbwj42ynkzNLSzu6NfSJJLd1OmOuCAAAAHam6SDjfcPnr9jluPXz33fV59NbfM4xsNBu587Z2W1/SE8kuWt21nYiAAAADoymA5ffzKDnyj8upZzayYBSytOT/OMMtub85lWHP2/4/NdNFcjhcvPMTM7Pz+f0FtuLTrdaOT8/n5tmZiZcGQAAAGyt6S1F/ymDW0I/JMlrSym3J/nx4RacT1BKuT6DJrkvH55fk/zIVad9xfDztzVcJ4fIQrudhXY7l/r9LPZ6uby2lpNTU1lot/VsAQAA4EBqNHCptb6xlPIfk/zLJJ+c5N8lOVdKeWuSP0vy4Qxue/y4JF+c5GEZrIhJkv9Ya72wPlcp5XOTPHn49vVN1snhNDc9LWABgE34RwkAOHiaXuGSWuu/KqV8IMm3ZXCXneuSPGX42Gg9aLk/yXfXWs9ddfxykvVtSfc2XScAwGG32OvlXLebi6urDzp2qtXK2U5HjzMA2Cel1jqeiUuZTfIvkjwvyWM3OeXdSV6T5IdrrctjKYJtlVLuvfHGG2+8916ZFgAcJnevrOTW5eVr3s1vvbG8XmcAsCdl+1MerPEVLuuGIco3JvnGUsqnJ3l0Bncd6if5y1rr/xrXtQEAjrLFXm/bsCVJriS5ZXk5N1x3nZUuADBhjQYupZSzw5d/Wmt95frnw3BFwAIA0IBz3e62Ycu6K0nu6HYFLgAwYU3fFvr2DO469JiG5wUAIIMGuZv1bLmWC6urudTvj6kiAGAzTQcuHxg+39fwvAAAZLCdaJLjAIDRNB24vGv4fLLheQEASHJ5bW2i4wCA0TQduPz3DLr3fkXD8wIAkOTk1Ggt+EYdBwCMpunA5Ucy2Fb0j0opT214bgCAY2/U5rea5gLAZDUauNRa/zLJi5J8OMlrSynfUEr55CavAQBwnM1NT+dUq7WrMadbrcxNT4+pIgBgM03fFvqe4cs/TPLUJD+Y5N+WUt6a5D1JPrLNFLXWenOTNQEAHDVnO52cWVra0a2hTyS5rdMZc0UAwNWa3sz7tUnq8PX683QG4ctOCVwAAK5hod3OnbOzuXV5+Zqhy4kkd83O2k4EAPug6R4uyaBp7sbHZp9t9QAAYAdunpnJ+fn5nN5ie9HpVivn5+dz08zMhCsDAJLmV7h8dsPzAWN2qd/PYq+Xy2trOTk1lYV22z5/gENiod3OQrvtv+UAcAA1GrjUWv+8yfmA8Vns9XKu283F1dUHHTvVauVsp2MJOsAhMTc9LWABgANmHFuKgAPu7pWVnFla2jRsSZKLq6s5s7SUe1ZWJlwZAADA0SBwgWNmsdfbtsliklxJcsvychZ7vUmUBQAAcKQIXOCYOdft7ug2oskgdLmj2x1jNQAAAEdT001zP0Ep5SFJvjDJY5OcTPKQ7cbUWn96nDXBcXap399yG9FWLqyu5lK/rzcAAADALowlcCmlfFaSlyf5qiSfvIuhNYnABcZk1O1Bi72ewAUAAGAXGg9cSilPTfLLSVpJStPzA6O7vLY20XEAAADHVaOBSynlZJL/muQRGbR/+H+T/GaSH8tg9coPJ1lOckOSZ2Ww3agmeWWSxSZrAR7s5NRof+VHHQcAAHBcNd00958n+bQMQpT/q9b6tbXWOzccX6y1/mit9VtrrV+U5O8n6SV5UZLUWl/RcD3ABgvt9kTHAQAAHFdNBy7PHj7fW2t91XYn11pfk+TvZrD16EdLKY9vuB5gg7np6ZxqtXY15nSrpX8LAADALjUduHx+Bqtb/tsWxx90l6Ja628neXUGzXX/ecP1AFc52+ns+C/+iSS3dTpjrAYAAOBoajpwecTw+V1XfX7/8HmrfyZf799ypuF6gKsstNu5c3Z227/8J5LcNTtrOxEAAMAImu6E+bHhnP/7qs8/mKSd5DFbjPvI8Hmr43AsXOr3s9jr5fLaWk5OTWWh3R7Ldp6bZ2bSue663NHt5sLq6oOOn261clunI2wBAAAYUdOBy3uSfG6SR131+Z8leVKSL91i3OyY6oFDYbHXy7luNxc3CT9OtVo5O4bwY6HdzkK7PbGQBwAA4DhpOuD4wwwClydc9flbknxJkueUUm6otf75+oFSyiMy6N1Sk9zXcD1w4N29spJbl5dzZYvjF1dXc2ZpKXfNzuammZnGrz83PS1gAQAAaFjTPVwuZHDHoadf9fnPDJ8fluRiKeWlpZQzpZSXJrk3yacPj2/VbBeOpMVe75phy7orSW5ZXs5irzeJsgAAANijpgOXXxo+P7GUMrf+Ya31dzIIXUqSxyb54SSvGz53hqe9K8n3N1wPHGjnut1tw5Z1V5Lc0e2OsRoAAACa0mjgUmt9V5JnJHlOkstXHb45yd3D1+Wqx71JFmqt/vmeY+NSv79pz5ZrubC6mkv9/pgqAgAAoCmNN6mttV7Y4vP7k9xSSvmuJF+R5G8l+XCS3621/lbTdcBBN+r2oMVeT88VAACAA27idwUaNsz9yUlfFw6ay2trEx0HAACwX47j3VHdhhn2ycmp0f76jToOAABg0hZ7vZzrdjdtp3Cq1crZTicL7fY+VDZ+jfZwKaVcKaWslVKet8txzyqlfLyU4p/uOTZG/Y/KUf2PEQAAcLTcvbKSM0tLW/auvLi6mjNLS7lnZWXClU1G03cpSgZNcEcdN+pYOHTmpqdzqtXa1ZjTrdaRX3YHAAAcfou9Xm5dXt72rqxXktyyvDxyj8uDbByBC7BDZzudHf8lPJHktk5njNUAAAA041y3u23Ysu5Kkju63TFWsz8OSuBy/fD5I/taBUzYQrudO2dnt/2LeCLJXbOzthMBAAAH3qV+f8ttRFu5sLqaS/3+mCraHwclcFkYPr93X6uAfXDzzEzOz8/n9Bbbi063Wjk/P5+bZmYmXBkAAMDujbo96KhtKxr5diellNNJTm9x+EWllC/abook00luTPKMJDXJb41aDxxmC+12FtrtY3mrNAAA4Gi5vDba/XBGHXdQ7eX+sk9PcnaTz0uSr9rlXCXJWpIf3EM9cOjNTU8LWAAAgEPt5NRoUcOo4w6qvW4pKlc9tvp8u8dbkzyv1vq7e6wHAAAA2Eej9p48aj0r9xIf/VSSN214X5L8WgZbg25L8hvbjL+S5ENJ7qu1fmAPdQAAAAAHxNz0dE61WrtqnHu61Tpyq/1HDlxqrX+e5M83flbK3yxyeXut9cIe6gIAAAAOqbOdTs4sLe3o1tAnktzW6Yy5oslr+i5Fz0jyFdl+dQsAAABwRC2027lzdnbb0OFEkrtmZ4/cdqJkb1uKHsSqFgAAACBJbp6ZSee663JHt5sLm2wvOt1q5bZO50iGLUnDgQsAAADAuoV2Owvtdi71+1ns9XJ5bS0np6ay0G4fuZ4tVxspcCml/P1a6y82Xcx+XQcAAAAYn7np6SMfsFxt1B4uv1BK+f1SyvMarWaolPL8Uspbk/z8OOYHAAAAGKdRtxR9OMkXJfnFUsofJfnpJP+51vruUQsppXxmkhcneUmSx2dwm+kPjTofAAAAwH4ZdYXLbJJXZxCKfH6S703SLaW8qZTy8lLKs0spj7rWBKWUR5VSnlNKub2U8qYk9yX57iRPGJ7yqgyCFwAAAIBDZaQVLrXW9yR5USnl3yf5ziTPziC8+fLhI0lSSnl/kvXHB5OcTPLI4ePqNsQlSU3yS0m+s9b61lFqAwAAANhve7pLUa313iTPLaXMJnlpkhcl+fQNpzwqg3DlauWq93+V5GeT/Kda6zv3UhMAwEbH8a4IAMD+a+S20LXW5STfXEr5liRfluSZSZ6c5IlJZvKJAUtN8p4kb0/y20n+R5LfqrVeaaIWAODgmmT4sdjr5Vy3m4urqw86dqrVytlOJwvtqxfcAgA0o5HAZd0wNPn14SNJUkr5pCSPSPKwJB9N8oFa6/1NXhcAONgmHX7cvbKSW5eXs9W/5lxcXc2ZpaXcNTubm2ZmGrsuAMC6UZvm7lit9f5a61/XWt89fBa2AMAxcvfKSs4sLW0atiQPhB/3rKw0cr3FXu+aYcu6K0luWV7OYq/XyHUBADYae+ACABxf+xF+nOt2t73exuve0e3u+ZoAAFcTuAAAYzPp8ONSv7/lSpqtXFhdzaV+f0/XBQC4msAFABiL/Qg/Rl0hY1sRANA0gQsAMBb7EX5cXlub6DgAgK0IXACAsdiP8OPk1Gg3YBx1HADAVgQuAMBY7Ef4MeqtpZu8JTUAQCJwAQDGZD/Cj7np6ZxqtXY15nSrlbnp6ZGvCQCwGYELADAW+xV+nO10dvwLzokkt3U6e7oeAMBmBC4AwNjsR/ix0G7nztnZba97Islds7O2EwEAYyFwAQDGZr/Cj5tnZnJ+fj6nt1hhc7rVyvn5+dw0M9PI9QAArqYlPwAwVjfPzKRz3XW5o9vNhdXVBx0/3Wrltk6n8ZUmC+12FtrtXOr3s9jr5fLaWk5OTWWh3dazBQAYu0YDl1LKzyb58Vrrm5qcFwA43PYz/JibnhawAAAT1/QKl69K8o9KKX+W5K4kP1Vr/V8NXwMAOKSEHwDAcTGOHi4lyeOSfG+Sd5VSfr6U8qwxXAcAAADgQGo6cHlSkh9P8sEMgpdPSvIPkry2lHJfKeU7SimPafiaAAAAAAdKo4FLrfWttdaXJplJcnOS38wgeClJbkjynUnuK6W8ppTy3FKKuyQBAAAAR85YAo9a60dqrT9Za31akrkk/zHJ+zIIXqaSPDfJa5L8RSnlXCnlhnHUAQAAALAfxr7CpNb6R7XWf5XkMUn+SZJfGx4qSR6d5NuT/Gkp5fWllH9QSnGragAAAOBQm9iWnlrrx2qtr6q1fmWSz8mgqe5KBsHLiSTPTPLzSd5dSvmeUspnTqo2AAAAgCbtSw+VWut9Sd6Y5C3rH+WBXi+fnuRbk/xJKeXHSimP2I8aAQAAAEY10cCllDJTSvm2UsqfJPnVJM9fP5Tkj5L8UJJ35YE7HN2S5PdKKZ82yTqvpZRyfSnl9lLK20opHyqlrJZSfreU8q9LKQ/dw7y3l1LqDh6f0+SfBwAAAGje2PullFJKkr+b5OuSPCfJQzIIVJLko0l+IcmP11rfPDz/mzNoqntbki9J8tlJzib5F+OudTvD5r5vStIZfvThJA/LoM4vSfLiUspCrbW3h8vcn+T91zi+toe5AQAAgAkYW+AyDCduTvJPM2iOmzwQtPxJkjuT/GSt9X0bx9Vaa5JfLqX8SpLXZdDb5dnjqnOnhs18fzmDsGUlyVfXWt8wvLX1C5PcleSLk/xMBgHTqH6z1vr0vVULAAAA7KdGA5dhKPH8DLYCLeSBvizJYGXGa5L8WK11cbu5aq1XSik/lUHg8llN1jmir0nyBcPXL6i1/lYyqDPJzw2Dl/+c5DnDVS7b/hkBAACAo6npFS7vSfKpw9frQcufZ7D64+5a61/tcr71rTUPaaC2vfqa4fMb18OWq7wqyXdnsAXqq5MIXAAAAOCYajpwWW9u+/Ekr03yY0leP9wmNIr3JHlFE4XtRSnl4UmeOnz7us3OqbXWUsrrk7w0yZlJ1QYAAAAcPONY4XJ3krtqre/Z62S11rdn0ANmvz0hD9zR6e3XOG/92GeUUh5Za71W89utzJVS3p7kcUmuZPC/6cUkP1prfesI8wEAAAAT1nTgcsOwp8lR8+gNr68VJG089uhc+25DW/nUJI9M8oEkJ5N83vBxcynle2qt37HTiUop9+7gtMePUCMAAABwDY0GLkc0bEmS6ze8/vA1ztt47Potz9rcO5P8mwwaC99Xa72/lPLQJE9P8j1JnpTk20spvVrr9+9ybgAAAPgbl/r9LPZ6uby2lpNTU1lotzM3Pb3fZR0pY7st9H4rpXxtkp/cwxTPrrW+vqFytlVrfeUmn30syflSysUMthV9aZLbSyk/UWtd3cGcT9runOEqmBtHKBkAAIBDZrHXy7luNxdX///27j3OsqugE/1vdSoQUkkXB1Fsg1IwagmN9pjMCCqmccoJ4vU6jC+I+GASkhlkrjNzfdyrktAmPhiU6wNFTUwMgwh+ZkTufHxgpAeSMD4GEmygwQadFFyhGREq1UnlAZVe94+zyz5p6l37POv7/Xz25+x99t5rr96r99nn/GrvtT/7J+WlMzO5dnY2853OEGo2efZtvghJ7u0ZP3+D5Xrn3bvuUttUa30wyY81kxek+8htAAAA2LKbTp7MZceOrRm2JMntS0u57Nix3Hzy5IBrNpkm9gqXJG9I8vu7WL/3f+DHesYvSvKedda5aJ112tD7KOqntFw2AAAAE+zo4mKuPnEim/UDcjrJVSdO5EnnnedKl12a2MCl1vpQkodaKu4D6f6/25fk6Vnn0dDNvCT5+A6fUAQAAACtu25hYdOwZdXpJNcvLAhcdsktRVtQa70/yX9vJr9xrWVKKSXJc5rJW/tQjWf2jN/dh/IBAACYQMeXl9e9jWg9ty0t5fjycp9qtDcIXLbutc3r15dSnrHG/O/ImVt9/tN2Cm7Cmo3mPzrJTzWTy0mObqd8AAAA9q6ji4sDXY8ugcvWvTbJe5OUJL9bSplPklLKvlLKdyS5sVnuj2qtnxWIlFKOlFJqM8yeNfvSUspbSynfU0p5Ys865zbbuSPJashzXa31nlb/ZQAAAEysUysrA12Prontw6VttdaVUsq3JHlbktkkby2l3J9uaHVes9i7k7xwB8WXdJ88tBriPJDulSwzSc5tljmd5BW11lfu9N8AAADA3rN/amc//Xe6Hl323jbUWhdKKV+R5IeSfGuSJyf5TJLj6T4V6dW11k/voOj3NmV+dZIvT/L4JI9Ncn+S96d7hcsNtdb37vofAQAAwJ6y085vdZq7O6XWOuw6MESllDsvvvjii++8885hV4U94vjyco4uLubUykr2T01lvtPJwenpYVcLAAAm2uF3v3tbHecenpnJ27/yK/tYo7GyYb+r63GFCzAQRxcXc93Cwpof8pfOzOTa2VkJOgAA9Mm1s7O57NixLT0ael+Sa2Zn+1yjyafTXKDvbjp5MpcdO7Zuon770lIuO3YsN588OeCaAQDA3jDf6eSGublNQ4B9SW6cm/PH0BYIXIC+Orq4mKtPnNg0ST+d5KoTJzx6DgAA+uTKAwdy66FDOTwzs+b8wzMzufXQoVxx4MCAazaZ3FIE9NV1Cwtbumwx6YYu1y8sSNMBAKBP5judzHc6+lYcAIEL0DfHl5e31TFXkty2tJTjy8s+7AEAoI8OTk/7zt1nbikC+mantwe5rQgAABh3Ahegb06trAx0PQAAgFEhcAH6Zv/Uzu5a3Ol6AAAAo0LgAvTNTju/1WkuAAAw7gQuQN8cnJ7Opes8cm49h2dmdN4FAACMPYEL0FfXzs5u+YNmX5JrZmf7WBsAAIDBELgAfTXf6eSGublNP2z2Jblxbs7tRAAAwEQQuAB9d+WBA7n10KEcXuf2osMzM7n10KFcceDAgGsGAADQHx4FAgzEfKeT+U4nx5eXc3RxMadWVrJ/airznY4+WwAAgIkjcAEG6uD0tIAFAACYeG4pAgAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGUCFwAAAICWCVwAAAAAWiZwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaNjXsCgD02/Hl5RxdXMyplZXsn5rKfKeTg9PTw64WAAAwwQQuwMQ6uriY6xYWcvvS0mfNu3RmJtfOzma+0xlCzWBtwkEAgMkhcAEm0k0nT+bqEydyep35ty8t5bJjx3Lj3FyuOHBgoHWDswkHAQAmjz5cgIlzdHFxw7Bl1ekkV504kaOLi4OoFqzpppMnc9mxY2uGLcmZcPDmkycHXDMAAHZD4AJMnOsWFjYNW1adTnL9wkIfawPrEw4CAEwugQswUY4vL697pcB6bltayvHl5T7VCNYnHAQAmFwCF2Ci7PQKAFcOMGjCQQCAySZwASbKqZWVga4HOyUcBACYbAIXYKLsn9rZw9d2uh7slHAQAGCyCVyAibLTR+d65C6DJhwEAJhsvrUBE+Xg9HQunZnZVt8Yh2dmcnB6uo+1og3Hl5dzdHExp1ZWsn9qKvOdzli3m3AQAGCyCVyAiXPt7GwuO3ZsS09/2ZfkmtnZPteI3Ti6uJjrFhbWDNEunZnJtbOzYxlCCAcBACabW4qAiTPf6eSGublNP+D2Jblxbm4sf6zvFTedPJnLjh1bN5S4fWkplx07lptPnhxwzdpx7ezslk/EwkEAgPEicAEm0pUHDuTWQ4dyeGZmzfmHZ2Zy66FDueLAgQHXjK06uriYq0+c2PRKpdNJrjpxYiyf3iMcBACYXG4pAibWfKeT+U5n4vr+2CuuW1jY0m1hSTd0uX5hYSwDiSsPHMjseefl+oWF3LbGlTyHZ2ZyzZjeNgUAsJcJXICJd3B6WsAyZo4vL2+rb5MkuW1pKceXl8eyrYWDAACTR+ACwMjZ6e1BRxcXxzqgEA4CAEwOfbgAMHJOrawMdD0AAGibwAWAkbN/amcXYO50PQAAaJvABYCRs9MOYnUsCwDAqBC4ADByDk5P59J1Hum9nsMzM/o/AQBgZAhcABhJ187ObvkktS/JNbOzfawNAABsj8AFgJE03+nkhrm5TU9U+5LcODfndiIAAEaKwAWAkXXlgQO59dChHF7n9qLDMzO59dChXHHgwIBrBgAAG/M4BwBG2nynk/lOJ8eXl3N0cTGnVlayf2oq852OPlsAABhZAhcAxsLB6WkBCwAAY0PgAsC2udoEAAA2JnABYMuOLi7muoWF3L609FnzLp2ZybWzszqvBQCA6DQXgC266eTJXHbs2JphS5LcvrSUy44dy80nTw64ZgAAMHoELgBs6ujiYq4+cSKnN1nudJKrTpzI0cXFQVQLAABGlsAFgE1dt7Cwadiy6nSS6xcW+lgbAAAYfQIXADZ0fHl53duI1nPb0lKOLy/3qUYAADD6BC4AbGintwe5rQgAgL1M4ALAhk6trAx0PQAAmAQCFwA2tH9qaqDrAQDAJBC4ALCh+U5noOsBAMAkELgAsKGD09O5dGZmW+scnpnJwenpPtUIAABGn8AFgE1dOzu75RPGviTXzM72sTYAADD6BC4AbGq+08kNc3ObnjT2Jblxbs7tRAAA7Hl6NARo2fHl5RxdXMyplZXsn5rKfKczEbfXXHngQGbPOy/XLyzktqWlz5p/eGYm18zOClsAACACF4DWmaOx8AAAIABJREFUHF1czHULC7l9jTDi0pmZXDsBYcR8p5P5TmdiQ6W9SFsCAPSHwAWgBTedPJmrT5zI6XXm3760lMuOHcuNc3O54sCBgdatHw5OT/tRPub2QkAIADBM+nAB2KWji4sbhi2rTie56sSJHF1cHES1YF03nTyZy44dWzNsSc4EhDefPDngmgEATA6BC8AuXbewsGnYsup0kusXFvpYG9iYgBAAYDAELgC7cHx5ed2rBNZz29JSji8v96lGsDEBIQDAYAhcAHZhp3/9d9UAwyAgBAAYHIELwC6cWlkZ6HqwGwJCAIDBEbgA7ML+qZ097G2n68FuCAgBAAZH4AKwCzt9bK7H7TIMAkIAgMERuADswsHp6Vw6M7OtdQ7PzOTg9HSfagTrExACAAyOwAVgl66dnd3yh+m+JNfMzvaxNrA+ASEAwOAIXLaolHJ+KeW5pZSXlVLeVEr5cCmlNsORFrfzhFLKq0opJ0opD5RSPlVKuaOU8uJSSmlrO0B75jud3DA3t+kH6r4kN87NuVqAoRIQAgAMhpuyt+6rkvxhPzdQSrkkyR8n+ZzmrfuSXJjkWc3w7aWUb6m1frqf9QC278oDBzJ73nm5fmEht63x2N3DMzO5ZnZW2MLQrQaEV584kdMbLCcgBADYHYHL9iwmuatn+Pkkn99GwaWUmSS/n27Y8ldJvqfW+q5SyqOSXNVs6zlJfiHJ97exTaBd851O5judHF9eztHFxZxaWcn+qanMdzpuyWCkCAgBAPpP4LJ1d9RaH9f7RinlFS2W/0PphjcPJPmmWuvdSdJczfIrpZT9SX46ydWllF+otX6wxW0DLTo4PS1gYeQJCAEA+kvgskW11of7vInvbV7fuBq2nOXVSX4syQVJXpjk5X2uDwB7gIAQAKA/dJo7Akopc0m+qJn8o7WWqbXel+SOZvKyQdQLAAAA2BmBy2h4es/4+zZYbnXe0/pYFwAAAGCX3FI0Gr6gZ/yjGyy3Om9/KeWC5qqXdZVS7tzCtr9sC8sAAAAA2+AKl9FwYc/4/Rss1zvvwnWXAgAAAIZqYq9wKaW8KMlv7qKI59Za39JSdYai1nrJZss0V8FcPIDqAAAAwJ7hCpfRcG/P+PkbLNc77951lwIAAACGamKvcEnyhiS/v4v1l9qqyBZ8rGf8oiSn1lnuoub11Gb9twAAAADDM7GBS631oSQPDbseW9T7ZKKnJ/nAOsutPs3o/f2tDgAAALAbbikaDR9M8pFm/BvXWqCUMp3k65rJWwdRKQAAAGBnBC4joNZak/ynZvIFpZTZNRZ7aZILkjyc5PWDqRkAAACwEwKXbSildEopj18dcmb/nd/7finlgjXWPVJKqc0wu0bxP5fk4+l2jPsHpZRLmvUeVUp5SZLrm+VuqLV+sO1/GwAAANAegcv2vDvJJ3qGL2ze/+Gz3v/l7RZca11K8s1JPpnkaUneVUo5leS+JK9J8qh0byX6D7v7JwAAAAD9JnAZIbXWO5McTPLzST6U5Nwky0nekeSqJM9tOgMGAAAARljpdh/CXlVKufPiiy+++M477xx2VQAAAGAUlZ2s5AoXAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGUCFwAAAICWCVwAAAAAWiZwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGUCFwAAAICWCVwAAAAAWiZwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGUCFwAAAICWCVwAAAAAWiZwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaNjXsCgDAqDq+vJyji4s5tbKS/VNTme90cnB6etjVAgBgDAhcAOAsRxcXc93CQm5fWvqseZfOzOTa2dnMdzpDqBkAAOPCLUUA0OOmkydz2bFja4YtSXL70lIuO3YsN588OeCaAQAwTgQuANA4uriYq0+cyOlNljud5KoTJ3J0cXEQ1QIAYAwJXACgcd3CwqZhy6rTSa5fWOhjbQAAGGcCFwBIt4Pc9W4jWs9tS0s5vrzcpxoBADDOBC4AkOz49iC3FQEAsBaBCwAkObWyMtD1AACYbAIXAEiyf2pqoOsBADDZBC4AkGS+0xnoegAATDaBCwAkOTg9nUtnZra1zuGZmRycnu5TjQAAGGcCFwBoXDs7u+UT474k18zO9rE2AACMM4ELADTmO53cMDe36clxX5Ib5+bcTgQAwLoELgDQ48oDB3LroUM5vM7tRYdnZnLroUO54sCBAdcMAIBx4tEKAHCW+U4n851Oji8v5+jiYk6trGT/1FTmOx19tgAAsCUCFwBYx8HpaQELAAA74pYiAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGVTw64AALt3fHk5RxcXc2plJfunpjLf6eTg9PSwqwUAAHuWwAVgjB1dXMx1Cwu5fWnps+ZdOjOTa2dnM9/pDKFmAACwt7mlCGBM3XTyZC47dmzNsCVJbl9aymXHjuXmkycHXDMAAEDgAjCGji4u5uoTJ3J6k+VOJ7nqxIkcXVwcRLUAAICGwAVgDF23sLBp2LLqdJLrFxb6WBsAAOBsAheAMXN8eXnd24jWc9vSUo4vL/epRgAAwNkELgBjZqe3B7mtCAAABkfgAjBmTq2sDHQ9AABg+wQuAGNm/9TUQNcDAAC2T+ACMGbmO52BrgcAAGyfwAVgzBycns6lMzPbWufwzEwOTk/3qUYAAMDZBC4AY+ja2dktf4DvS3LN7GwfawMAAJxN4AIwhuY7ndwwN7fph/i+JDfOzbmdCAAABkzgAjCmrjxwILceOpTD69xedHhmJrceOpQrDhwYcM0AAACPrAAYY/OdTuY7nRxfXs7RxcWcWlnJ/qmpzHc6+mwBAIAhErgATICD09MCFgAAGCFuKQIAAABomcAFAAAAoGUCFwAAAICWCVwAAAAAWiZw2aJSyvmllOeWUl5WSnlTKeXDpZTaDEdaKP9IT3kbDV/cwj8HAAAA6CNPKdq6r0ryhwPYzmeSfGqD+SsDqAMAAACwCwKX7VlMclfP8PNJPr/lbfxprfXZLZcJAAAADJDAZevuqLU+rveNUsorhlUZAAAAYHTpw2WLaq0PD7sOAAAAwHgQuAAAAAC0TOAyeg6WUt5XSrm/lHJfKeVEKeXGUspXDrtiAAAAwNbow2X0PD7J45Lck2R/ki9thitLKT9da33ZVgsqpdy5hcW+bEe1BAAAANblCpfR8aEkP5JkLsl5tdbPSTKd5DlJ7kxSkvx4KeUHh1dFAAAAYCsm9gqXUsqLkvzmLop4bq31LS1VZ1O11tev8d6nk9xaSrk9ye1J/mmSI6WU36i1Lm2hzEs2W6a5CubiHVQZAAAAWIcrXMZArfXBJD/WTF6QZH6I1QEAAAA2MbFXuCR5Q5Lf38X6m15BMmB/1jP+lKHVAgAAANjUxAYutdaHkjw07HoAAAAAe8/EBi4T6Jk943e3WO7sBz7wgVxyyabdvQAAAMCec9ddd72+1vrC7a4ncBkBpZRSa60bzH90kp9qJpeTHG1x86ceeOCB3HXXXQstltlPq4+x/quh1oKd0HbjSbuNL203nrTb+NJ240m7jS9tN572VLsJXLahlNJJck7PW6udDp9fSnl8z/sP1lrvO2vdI0le3kw+uda60DP70lLKNUlem+Rttda/bdY5N8mlSX4m3ScUJcl1tdZ7WvjnJElqrU9uq6xBaJ6qtKUnMDFatN140m7jS9uNJ+02vrTdeNJu40vbjae91m4Cl+15d5InrfH+DzfDqtcmedE2yi3pPnloPklKKQ+keyXLTJJzm2VOJ3lFrfWV26syAAAAMGgCl9Hw3iQ/lOSrk3x5kscneWyS+5O8P8kdSW6otb53aDUEAAAAtkzgsg211tldrHskyZF15n0yyat2WjYAAAAwWvZtvggAAAAA2yFwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaVmqtw64DAAAAwERxhQsAAABAywQuAAAAAC0TuAAAAAC0TOACAAAA0DKBCwAAAEDLBC4AAAAALRO4AAAAALRM4AIAAADQMoELQ1FKOb+U8txSystKKW8qpXy4lFKb4UiL23lCKeVVpZQTpZQHSimfKqXcUUp5cSmlbGH9f1RK+fVSyt2llAdLKZ8opfxxKeXb2qrjOCqlXFhKOVJKeW8p5b5SylIp5Z2llB8spTxqh2XO9vwf2Mrwm2uUccsW153a/V4YT/1ou6bcI1vc91+8STkXl1J+q5Tyt6WUh0opJ0spv1dK+Wc7rdsk6GO7XVRK+f5Syn8upfx18zn5QPOZ94bN9ntb7T6O+tUmTdnOXX3Up3OYY6nP+tRuzl0D0HbbFd8Z+6oM4HfanjrP1VoNhoEPSZ6dpK4zHGlpG5ck+fuecu9N8pme6bckedQG639TkuWe5ZeSPNwzfXOSMux9OYS2e1KSu3v2w3KSB3um70rS2UG5X5jk45sM9/Rs5/vXKOOWZt4Dm5RzzrD34yS1XVP2kaaMT2+y72c3KOPFZx2j9yQ53fZnw7gNfT7mevfvatn3n/XeTesdM220+zgOfT6WnLvGrO0cS+PZbm3t9zh3Dbzt4jtjv9vs2Wd9drX6Oy177Dw39AoY9ubQHMifSvLWJK9M8oIkJ1s8kGd6yvtAkn/SvP+oJC9tTqw1yWvWWf/JSe5rlnlHki9t3r8gyU/0HNA/Mux9OeB2m0rynubf/rEk39C8vy/J85Ocaub9QZ+2/+qm/PuTPHaN+asnz1uGva9Gbeh32+XMl9a373D9r06y0pTxe0me2Lz/OUl+reeY+85h78tJabcks826b03yvUm+oKfspyV5c89+v74f7T6OQ5/bxLlrDNvOsTSe7dbGfnfuGl7bbWHbvjPufN89O336nbYXz3NDr4Bhbw5ZIy1OstDGgdyUdX3Ph+yT15j/o838ldUD9az5r2vmn1znQ/rXcyZR3dFfMcdxSHJlzwfZV68x//Ke+fMtb/u85sO/JnndOss4eQ6p7bL7L613NOu/J8m5a8x/SzP/7rU+PyZ16Ge7NV96Lt5gfknyRznz16fz2m73cRz63CbOXWPYdo6l8Wy3Nva7c9fw2m6T7frOuLv917ffaXvxPKcPF4ai1vpwnzfxvc3rG2utd68x/9XppqPnJHlh74xSynSS1fv/frXWes8a6/9M87o/yfN2X92x8X3N69tqrX+2xvw3pvulIjnTBm351iSdZvw3Wi57Lxhm222olPKUJM9qJn+u1vqZNRZbPeZmk1w6iHqNiL61W611qdZ61wbza7qX5Sbdvxw9dTvlT7B+HkvOXf3Vl7ZzLPXdSJ6/nLu2ZFht5zvjLvT5d9qeO88JXJg4pZS5JF/UTP7RWsvUWu9L968SSXLZWbOfleQxm6y/kO5lcGutP5FKKecn+dpmcr39UtP9a07S/n65snn9UK31tpbLnmgj0Hab+ec9429ZZ5l3pPuX4cQx9w8G0G4P9oyf04fyx0o/28S5q79G4HhyLO3ACLTbRpy7NjDktvOdcQTt1fOcwIVJ9PSe8fdtsNzqvKftcv2DW6zXuHtqznxmbGW/fH4p5XFtbLj5K9LXN5M3bWGV+VLKB5tey081veL/QinlS9qozxgaZNsdLKW8r5Ryf/MkghOllBtLKV+5wTqrx9zf1Vr/bq0Fmr+2/NXqNnZYt3EztGOux7Ob108n+eAGy+2k3cdRP9vEuau/hn08Pbt5dSxtz6DazbmrfUM55nxnHGl78jwncGESfUHP+Ec3WG513v5SygVrrL9Ya31gC+t/wQbLTJLt7tez19mNK9K9B34lyWu3sPwTkzwl3ftDz0/3A/rfJXlfKeUlLdVpnAyy7R6f7pesB5I8OsmXpvsEhztLKT+5Sf02qlvvfMfcI/XjmEsp5clJ/k0z+Tu11lMbLL6Tdh9H/WwT567+Gtrx5FjalUG1m3NX+4Z1zPnOOLr25HlO4MIkurBn/P4Nluudd+Ea4xut2zv/wg2Xmhy73a87Uko5J8mLmsk/qLV+fIPF70ryb9O9V/rRtdbHpXsP57cl+Zt0e0B/TSnl29YtYTINou0+lORHksyl2yHk5ySZTvKcJHem++Xnx0spP7hB/RxzjzSUYy5JSimPSfKf0/3y+fdJ/u91Ft1Nu4+jfraJc1d/Desc5ljanX63m3NX/wz8mPOdceTtyfOcwIUtKaW8qJRSdzF847D/DXvRhLTbNya5qBnfsOOzWusv1Vp/pdb64dUOv2qt99da35TkGTnTMdurSimlbzVuwbi1Xa319bXWn621fnC148Ba66drrbeme8/tO5tFj5RSZgZZt0Eat3Zb598wleS3k1yS5DNJXlhr/dhay2p3WJ9jafTZ7xNnT35nZLQJXJhE9/aMn7/Bcr3z7l1jfKN1e+ffu+FSk2O3+3WnXty8fjTrdJC1FbXWTyb56WbySUn20v3ww2q7JEmt9cEkP9ZMXpBk/qxFHHNrG3i7NX8dfH26PfuvJPmu5ofHtm2h3cdRP9vEuau/Bno8OZZaM7Tzl3PXrg2j7XxnHG178jw3NewKMDbekOT3d7H+UlsV2YLevx5dlGS9e6VXE/BTTY/YZ6/fKaU8ZoN7BC86a/lR1Ga7nb1f37POOhf1jO9q35RSnpDkm5vJW1p4TF3vIwmfku7lpKNqrNtuDWfv+16r27ooG3PMra2Vdmt+IP5Wku9M8nCS7661/pedltfYqN3HUT/bxLmrvwZ2PDmWWjXs89deOXf1w6DPYXv5O+O42JPnOYELW1JrfSjJQ8Ouxxb19lr99Jx5NNjZVnu6fv8m678za1td//i2ajdALbfbB5KcTvfKuKdn/b8crO6Xj9daP7XLbX5vup9TNcnNuyxrrExA223H6jH3eaWUz621fuLsBZofMF/WTDrmHmnX7dbz1/jn58wPxN/ZSVkTrp9t4tzVXwM5nhxLrRvl89fEnLv6ZNBtt2e/M46RPXmec0sRk+iDST7SjK/ZH0IpZTrJ1zWTZ1/i+450e6nfaP0npdub/VrrT6Ra6/1J/nszud5+Kel2NJe0s1+ubF7fVmv9ny2U98ye8bvXXWrCDKntzrbRvv+TnvH1+jD52pzp/Mwx12ij3ZofBL+dR/5AfONOylrDRB1zfW4T564+GsTx5Fhq3wicv5y7dmgIbec74+jbm+e5WqvBMBJDkoV0U+kjLZR1fVPWcpLZNeb/SDN/JcmXrjH/dc38jyWZWWP+a5r5p5J0hr3vBthGVzb/7tNJnrHG/O9s5tck87vc1rN6yrp8C8uXTeY/Lt1e52u6H/b7hr0/J6XttrDvH53kz5uy70vy2DWWuaOZ/5dJzl1j/h828xeSnDPs/TkJ7dasf06SNzbrfybJ8wfZ7uM49PlYcu4a37ZzLI1Zuzl3jW/brVGO74z9b8uFtPA7bS+e54ZeAcPeHZJ0kjy+Z/hIc4C88qz3L1hj3SM9H6yza8yfSXKymX88ySXN+49K8pJ0L/mvSV6zTt2e3Jxca5Lbk3xJ8/50kmubE0dN8iPD3o8DbrOpdO/BrUn+dvXkmO7Vct+Rbv8TNckfrrP+hu121rK3NMt9Mt3H9W1Wt+9J8qZ0H+f3eT3vPybdTgtP9Gx7y1+EJ2XoZ9slOZzkrU0bPLHn/XPT7WTwf/Ssu+Yxk+RrmpNrTfK7SS5q3n9cz8mzJvnOYe/LCWq3c9Ltc2b1B+J3bLNuu273cRx20yabfQbGuWss286xNLbt5tw1pm23xrK3xHfGtttuR7/TNmu37MHz3NArYNi7Q84kpZsNt6yx7qYfwuk+hvHve5Y7leTTPdN/vNGHcpJvSjd9XV3+np6Tak33/tANE/JJHJLMpntp5ep+WE738r7V6buyTqK81ZNnkv09+/4Xt1ivF531/+a+pv172+zBJN8/7H04aW2X5Nln7fv7k3zirOPt4SQ/tUn9Xpzuj5XVdRZ7Tpw1LVz9No5DH9vt0p55n07y8U2G55+1fivtPo7DTttkK5+Bce4au7ZzLI1tu7Wy3+PcNfC2O2s53xn7024LZ+2j9YZbtttu2WPnOX24MLFqrXcmOZjk55N8KN2/WCyne//fVUmeW7sdXK63/h8m+YokN6b7oXNeuifRP0ny7bXWK2pz1O8ltdaFdPfLdel2XlXT/aJxZ5IfSvLMWuviLjfzgpx5pNtvbHGdtyX58XSfEPM3TZ1m0v0Qf2eS/5jkqbXW1+yybmOrj2333mb93033/twHkjy2eT2W5JeT/ONa649vUr/fSPKMdPtA+Gi6/wf+Lsmb0/2r2JEd1G3s9bHder8DnJvkCZsMjzlr/VbafRz183PQuau/+tR2jqU+61O7OXcNwAC+N/rOOGb22nmujFBdAAAAACaCK1wAAAAAWiZwAQAAAGiZwAUAAACgZQIXAAAAgJYJXAAAAABaJnABAAAAaJnABQAAAKBlAhcAAACAlglcAAAAAFomcAEAAABomcAFAAAAoGUCFwAAAICWCVwAgD2plPLEUsp9pZRaSvm2NebPNvNqKeWWIVRxzymlfHezv+8ppXzesOsDALshcAEA9qqfSzKd5D1J3jTkuoysUsrzSilHmuGxfd7cG5J8MMlMkp/p87YAoK8ELgDAnlNKuSTJ85vJI7XWOsz6jLjnJXl5M/Q1cKm1PpzkJ5vJF5VSntrP7QFAPwlcAIC96Cea1w8kefNaC9RaF2qtpRleNLCa8dtJPpzu99SXD7kuALBjAhcAYE8ppXxZkm9qJl/n6pbR0lzl8vpm8ttLKV80zPoAwE4JXACAvebfJClJas78sGe0rLbLOUmuHmZFAGCnBC4AwFCUUg6XUh5unkrzkY06ZC2lPLmUstQse18pZW6H2zwnyeXN5DtqrR/ZYNlNn1JUSnn76jI9772glPInpZSPl1IeKqV8uJRyS3NlzUZ1e0RZpZSpUspLSinvKKV8opTyQCnlg6WUXyylfOEmZS00ZS1stNxGyzZ1rkm+r+ftu3v2ybr7ppRycSnl10op7y2lnCqlfKaU8nellPeXUt5SSrmmlPIl69Wp1vr+JH/ZTH53KaVs9u8AgFEjcAEAhqLWeluSVzSTX5jkhrWWK6VMpduvx/7mrX9faz2xw81+TZLVxw2/bYdlrKmUcl4p5ffSfdLONyR5QpJHJfmidEOLvyylPHeLZXWSvD3Ja5J8bZLHJzkvyZck+YEkx0sp37RuAUNUSjmS5F1J/nWSpye5MMlUks9N8tQkz0lyXZKf3aSo1fZ5UpJD/agrAPTT1LArAADsaS9PMp/kGUm+o5RyRa315jWWeWYz/ru11t/Yxfb+ec/4/9hFOWu5Od0n+tyZ5I1JPpJuUPLCdIOeRyf5rVLKXK3177dQ1tcmeX+S16bbieznp3t1zjPSDTHeVEp5Vq31XS3/O1b9UrodCv9Akq9v3vvXSf7urOX+4SqhUsq/yJmObh9IN3z68ySfSjcwemKSf5JHtsN6/rxn/Dk5c8ULAIwFgQsAMDS11pVSynel+2P6wiS/VEq5o9b6oSQppXxdkh9tFv//kly1y00+o2e87cDl8iQ/leSa3o54Sym/luR30w1jHpfkiiSv3KSs56V7Vc+Laq2f6Snrl5L8xyQ/nG6Ac3Mp5VA/Ov6ttd6V5K5SyvN63r611rqwwWqr7fNwkm+otf7pWguVUs5L8hWbVOEvesafse5SADCi3FIEAAxVrfV/JnlpMzmd5LdLKec2fbr8Vrodp55O8t211sVdbu7pzeunaq2f2GVZZ/tvtdaXnR1+1FpPpxuQrHrOFsq6O8mVvWFLU1ZN8n/lzNUfX57ksp1XuXVf3LweXy9sSZJa64O11g0Dr1rrh9O9SibZPJwBgJEjcAEAhq7W+rp0r+hIurecXJ/k19Pt/yRJfqbWevtutlFKeVSSA83kp3ZT1jp+cb0Ztda/TvcKnSR52hbKek2t9cF1yqpJ/p+et/7llmvYf/c3r08spcy0UN5qwPaFOs4FYNwIXACAUfGSdK/sSLpXcXxnM/7nSY60UP5j030cdNKfwOXPN5n/0ea1s4Wyjm4y/7/1jP/TLZQ3KH/SvD4uyW2llMtLKfs3WmETn2xeH5Xu1U8AMDYELgDASKi1nkq3g9mVnrdPJXlhrXVl7bW25dE94/e2UN7ZNusI96E16rGev95oZq31k0nuaSa/YAvlDcor0u3oN+k+Wei3k3yqlPKu5nHWzyulPGYb5Z3qGd/OegAwdAIXAGCU/G2S5Z7pO5s+XtrwUM/4bq66WFPTV0tb7t98kX/YTxe0uN1dafrYeWa6nQf/r+btc5Jcku7Tjn4vyf8qpVzX3OK1md7bkh5YdykAGEECFwBgJJRS9qXbSW7vj+yvL6W8pKVN3JNktUPbx7VUZr+cv4VlVm+xuW+X22r1+2Ct9d5a68vSvfLm4iT/R5LfyZkrgC5Mck2S/7qFfllW2+nTeWQQBwAjT+ACAIyKH01yaTN+NMlSM/6qUspTd1t4rfXTST7WTI564PLFG80spXxOun3SJGf+Tb1Wr+bZ8CqSJvDoy76otZ6utb671vrLtdYXJHlCuh38rvaf85wk/9smxazW7SP9ePQ1APSTwAUAGLpSyjNypmPcjyV5frqd6CbdvjveUErZSt8nm3lf89oppXxuC+X1yz/bZP7X94y/c435q/27PL6Ucu4G5Tw9m3dG23ur1I6fFNQEMG9Ocm3P289ab/lSymyS85rJ9+x0uwAwLAIXAGCoSikXJnl9kql0b/n5vlrrJ2utb0jePqfKAAADsUlEQVTyumaxQ+l2yLpbf9Ez/owWyuuX798kYPoPPeNvWmP+ase15yb5ug3K+YEt1KX3lqU2nhS00DM+tcFyve3zF+suBQAjSuACAAzbryT5R834q2qtb+2Z99Ikq53m/rtSynN2ua0/6Rn/ql2W1U9PSXJjKeURgUTp+ukkX9O89Z488t+06i0949evFd6UUl6c5MVbqMvdPeMXb7RgKeWGUsrTN5g/leSqnreObVBcb+DyxxvWEABG0EZ/VQAA6KtSyuVJvqeZfHeSH++dX2u9t5TyXUneke73lltKKV9Ra/3EDjf5Z+k+PecJSZ69wzIG4c3p7pevLKW8NslH0q3z5Um+ulnmoSRXrtO3yZvTfbT0F6cbzryzlHJTurdrfX6S56V729Id6YZdGz1a+mjP+CubW7FO5Mzjuz9aa31vM35VkqtKKceTvC3dW7g+le6VMU9J8oIkX9Is+8Ek/2WD7a7eNrVQa90omAGAkSRwAQCGoumj41ebyfuTfFfTse0j1Fr/opRyJMlPphsW3Jzkf9/JNmutD5dS3pDk3yd5VinlSbXWD++krD77V0k+N8nXJvnZNebfm+TyWuu71lq51vpQKeUF6V790kny5Ul+4azF/iLJt2XtPmB6y3pPs88uTzf0+bmzFnltkhed9d7BZljPe5L8i1rrmo96bjpJ/sfN5G9tVD8AGFVuKQIABq6Uck4e+Qjo/7PW+lcbrPIzSW5vxr+5lPJvd7H5X023r5iS5IW7KKdvaq33pHuFx0uT/GmST6Z7RcvfJHl1koO11j/YpIw70w1aXp3u1S4PJllM9yqflyb5um1cKfQ96XZi/PZ0H++8ss5yFyW5MsktSe5qtvdwkgfS7bvl99Ld5xfXWhc22N53N68PJ7lxi3UEgJFSPGEPANhrSin/b5JvSfJXSZ42Co8cLqW8PcnhJKm17vhpQOOuCeP+Oslskt9pHikNAGPHFS4AwF70E+le5fJlSf7lkOvCI12ebthyOt12AoCxJHABAPacWutdSX6nmXx5KWXPXlEySpqrW17WTN5Sa/3AMOsDALshcAEA9qofSrKc5CuSfOuQ60LX5Unmkiwl+dEh1wUAdkUfLgAAI0AfLgAwWVzhAgAAANAyV7gAAAAAtMwVLgAAAAAtE7gAAAAAtEzgAgAAANAygQsAAABAywQuAAAAAC0TuAAAAAC0TOACAAAA0DKBCwAAAEDLBC4AAAAALRO4AAAAALRM4AIAAADQMoELAAAAQMv+f41dX0UEv4teAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "image/png": {
              "width": 558,
              "height": 341
            },
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# @markdown #### Generar el conjunto de datos de muestra\n",
        "set_seed(seed=SEED)\n",
        "n_samples = 32\n",
        "inputs = torch.linspace(-1.0, 1.0, n_samples).reshape(n_samples, 1)\n",
        "noise = torch.randn(n_samples, 1) / 4\n",
        "targets = torch.sin(pi * inputs) + noise\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(inputs, targets, c='c')\n",
        "plt.xlabel('x (inputs)')\n",
        "plt.ylabel('y (targets)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZuUzg9_6eKig"
      },
      "source": [
        "Vamos a definir una red neuronal muy amplia (512 neuronas) con una capa oculta y la función de activación `nn.Tanh()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "NYC_Tf6-eKig"
      },
      "outputs": [],
      "source": [
        "class WideNet(nn.Module):\n",
        "  \"\"\"\n",
        "   A Wide neural network with a single hidden layer\n",
        "   Structure is as follows:\n",
        "   nn.Sequential(\n",
        "        nn.Linear(1, n_cells) + nn.Tanh(), # Fully connected layer with tanh activation\n",
        "        nn.Linear(n_cells, 1) # Final fully connected layer\n",
        "    )\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializing the parameters of WideNet\n",
        "\n",
        "    Args:\n",
        "      None\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    n_cells = 512\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(1, n_cells),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n_cells, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of WideNet\n",
        "\n",
        "    Args:\n",
        "      x: torch.Tensor\n",
        "        2D tensor of features\n",
        "\n",
        "    Returns:\n",
        "      Torch tensor of model predictions\n",
        "    \"\"\"\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TH0Z01jJeKih"
      },
      "source": [
        "Ahora podemos crear una instancia de nuestra red neuronal e imprimir sus parámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "8MnP2BH5eKih"
      },
      "outputs": [],
      "source": [
        "# Creating an instance\n",
        "set_seed(seed=SEED)\n",
        "wide_net = WideNet()\n",
        "print(wide_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "B4Y13FPPeKih"
      },
      "outputs": [],
      "source": [
        "# Create a mse loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Stochstic Gradient Descent optimizer (you will learn about momentum soon)\n",
        "lr = 0.003  # Learning rate\n",
        "sgd_optimizer = torch.optim.SGD(wide_net.parameters(), lr=lr, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TvKTingPeKii"
      },
      "source": [
        "El proceso de entrenamiento en PyTorch es interactivo - puedes realizar iteraciones de entrenamiento como quieras e inspeccionar los resultados después de cada iteración. \n",
        "\n",
        "Vamos a realizar una iteración de entrenamiento. Puedes ejecutar la celda varias veces y ver cómo se actualizan los parámetros y se reduce la pérdida. Este bloque de código es el núcleo de todo lo que viene: por favor, asegúrese de ir línea por línea a través de todos los comandos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ZKdaX_jseKii"
      },
      "outputs": [],
      "source": [
        "# Reset all gradients to zero\n",
        "sgd_optimizer.zero_grad()\n",
        "\n",
        "# Forward pass (Compute the output of the model on the features (inputs))\n",
        "prediction = wide_net(inputs)\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_function(prediction, targets)\n",
        "print(f'Loss: {loss.item()}')\n",
        "\n",
        "# Perform backpropagation to build the graph and compute the gradients\n",
        "loss.backward()\n",
        "\n",
        "# Optimizer takes a tiny step in the steepest direction (negative of gradient)\n",
        "# and \"updates\" the weights and biases of the network\n",
        "sgd_optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Bi8bLHaceKij"
      },
      "source": [
        "### Ejercicio: Bucle de formación\n",
        "Utilizando todo lo que hemos aprendido hasta ahora, te pedimos que completes la función de `entrenamiento` que aparece a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "a7-st9AAeKij"
      },
      "outputs": [],
      "source": [
        "def train(features, labels, model, loss_fun, optimizer, n_epochs):\n",
        "  \"\"\"\n",
        "  Training function\n",
        "\n",
        "  Args:\n",
        "    features: torch.Tensor\n",
        "      Features (input) with shape torch.Size([n_samples, 1])\n",
        "    labels: torch.Tensor\n",
        "      Labels (targets) with shape torch.Size([n_samples, 1])\n",
        "    model: torch nn.Module\n",
        "      The neural network\n",
        "    loss_fun: function\n",
        "      Loss function\n",
        "    optimizer: function\n",
        "      Optimizer\n",
        "    n_epochs: int\n",
        "      Number of training iterations\n",
        "\n",
        "  Returns:\n",
        "    loss_record: list\n",
        "      Record (evolution) of training losses\n",
        "  \"\"\"\n",
        "  loss_record = []  # Keeping recods of loss\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    #################################################\n",
        "    ## Implement the missing parts of the training loop\n",
        "    # Complete the function and remove or comment the line below\n",
        "    raise NotImplementedError(\"Training loop `train`\")\n",
        "    #################################################\n",
        "    ...  # Set gradients to 0\n",
        "    predictions = ...  # Compute model prediction (output)\n",
        "    loss = ...  # Compute the loss\n",
        "    ...  # Compute gradients (backward pass)\n",
        "    ...  # Update parameters (optimizer takes a step)\n",
        "\n",
        "    loss_record.append(loss.item())\n",
        "  return loss_record\n",
        "\n",
        "set_seed(seed=2021)\n",
        "epochs = 1847 # Cauchy, Exercices d'analyse et de physique mathematique (1847)\n",
        "## Uncomment to run\n",
        "# losses = train(inputs, targets, wide_net, loss_function, sgd_optimizer, epochs)\n",
        "# ex3_plot(wide_net, inputs, targets, epochs, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "teRdEUfNeKij"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial1_Solution_66cb245e.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1696.0 height=544.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/W1D2_Tutorial1_Solution_66cb245e_1.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TgijVo_feKik"
      },
      "source": [
        "---\n",
        "# Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cBBhN7KLeKik"
      },
      "source": [
        "En este tutorial, hemos cubierto uno de los conceptos más básicos del aprendizaje profundo; el grafo computacional y cómo una red aprende mediante el descenso de gradiente y el algoritmo de retropropagación. Todo ello lo hemos visto utilizando módulos de PyTorch y hemos comparado las soluciones analíticas con las proporcionadas directamente por el módulo de PyTorch."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IA-UNJBG-Semana2-1/3",
      "provenance": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}